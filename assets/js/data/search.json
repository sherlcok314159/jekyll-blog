[ { "title": "Diving in distributed training in PyTorch", "url": "/posts/Diving-in-distributed-training/", "categories": "Training, PyTorch", "tags": "Multi-gpus, DataParallel, Distributed Training", "date": "2022-11-20 21:37:00 +0800", "snippet": "鉴于网上此类教程有不少模糊不清，对原理不得其法，代码也难跑通，故而花了几天细究了一下相关原理和实现，欢迎批评指正！ 关于此部分的代码，可以去这里查看在开始前，我需要特别致谢一下一位挚友，他送了我双显卡的机器来赞助我做个人研究，否则多卡的相关实验就得付费在云平台上跑了，感谢好朋友一路以来的支持，这份恩情值得一辈子铭记！这篇文章作为礼物赠与挚友。Why Parallel我们在两种情况下进行并行化训练1： 模型一张卡放不下：我们需要将模型不同的结构放置到不同的GPU上运行，这种情况叫ModelParallel(MP) 一张卡的batch size(bs)过小：有些时候数据的最大长度调的比较高（e.g., 512），可用的bs就很小，较小的bs会导致收敛不稳定，因而将数据分发到多个GPU上进行并行训练，这种情况叫DataParallel(DP)。当然，DP肯定还可以加速训练，常见于大模型的训练中这里只讲一下DP在pytorch中的原理和相关实现，即DataParallel和DistributedParallelData Parallel实现原理实现就是循环往复一个过程：数据分发，模型复制，各自前向传播，汇聚输出，计算损失，梯度回传，梯度汇聚更新，可以参见下图2：pytorch中部分关键源码3截取如下：def data_parallel(\tmodule, \tinput, \tdevice_ids, \toutput_device=None): if not device_ids: return module(input) if output_device is None: output_device = device_ids[0] # 复制模型 replicas = nn.parallel.replicate(module, device_ids) # 拆分数据 inputs = nn.parallel.scatter(input, device_ids) replicas = replicas[:len(inputs)] # 各自前向传播 outputs = nn.parallel.parallel_apply(replicas, inputs) # 汇聚输出 return nn.parallel.gather(outputs, output_device)代码使用 因为运行时会将数据平均拆分到GPU上，所以我们准备数据的时候， batch size = per_gpu_batch_size * n_gpus同时，需要注意主GPU需要进行汇聚等操作，因而需要比单卡运行时多留出一些空间import torch.nn as nn# device_ids默认所有可使用的设备# output_device默认cuda:0net = nn.DataParallel(model, device_ids=[0, 1, 2], output_device=None, dim=0)# input_var can be on any device, including CPUoutput = net(input_var)接下来看个更详细的例子4，需要注意的是被DP包裹之后涉及到模型相关的，需要调用DP.module，比如加载模型class Model(nn.Module): # Our model def __init__(self, input_size, output_size): super(Model, self).__init__() # for convenience self.fc = nn.Linear(input_size, output_size) def forward(self, input): output = self.fc(input) print(\"\\tIn Model: input size\", input.size(), \"output size\", output.size()) return outputbs, input_size, output_size = 6, 8, 10# define inputsinputs = torch.randn((bs, input_size)).cuda()model = Model(input_size, output_size)if torch.cuda.device_count() &gt; 1: print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\") # dim = 0 [6, xxx] -&gt; [2, ...], [2, ...], [2, ...] on 3 GPUs model = nn.DataParallel(model)# 先DataParallel，再cudamodel = model.cuda()outputs = model(inputs)print(\"Outside: input size\", inputs.size(),\t \"output_size\", outputs.size())# assume 2 GPUS are available# Let's use 2 GPUs!# In Model: input size torch.Size([3, 8]) output size torch.Size([3, 10])# In Model: input size torch.Size([3, 8]) output size torch.Size([3, 10])# Outside: input size torch.Size([6, 8]) output_size torch.Size([6, 10])# save the modeltorch.save(model.module.state_dict(), PATH)# load againmodel.module.load_state_dict(torch.load(PATH))# do anything you want如果经常使用huggingface，这里有两个误区需要小心：# data parallel object has no save_pretrainedmodel = xxx.from_pretrained(PATH)model = nn.DataParallel(model).cuda()model.save_pretrained(NEW_PATH) # error# 因为model被DP wrap了，得先取出模型 #model.module.save_pretrained(NEW_PATH)# HF实现貌似是返回N个loss（N为GPU数量）# 然后对N个loss取meanoutputs = model(**inputs)loss, logits = outputs.loss, outputs.logitsloss = loss.mean()loss.backward()# 返回的logits是汇聚后的# HF实现和我们手动算loss有细微差异# 手动算略好于HFloss2 = loss_fct(logits, labels)assert loss != loss2True显存不均匀了解前面的原理后，就会明白为什么会显存不均匀。因为GPU0比其他GPU多了汇聚的工作，得留一些显存，而其他GPU显然是不需要的。那么，解决方案就是让其他GPU的batch size开大点，GPU0维持原状，即不按照默认实现的平分数据首先我们继承原来的DataParallel（此处参考5），这里我们给定第一个GPU的bs就可以，这个是实际的bs而不是乘上梯度后的。假如你想要总的bs为64，梯度累积为2，一共2张GPU，而一张最多只能18，那么保险一点GPU0设置为14，GPU1是18，也就是说你DataLoader每个batch大小是32，gpu0_bsz=14class BalancedDataParallel(DataParallel): def __init__(self, gpu0_bsz, *args, **kwargs): self.gpu0_bsz = gpu0_bsz super().__init__(*args, **kwargs)核心代码就在于我们重新分配chunk_sizes，实现思路就是将总的减去第一个GPU的再除以剩下的设备，源码的话有些死板，用的时候不妨参考我的6def scatter(self, inputs, kwargs, device_ids): # 不同于源码，获取batch size更加灵活 # 支持只有kwargs的情况，如model(**inputs) if len(inputs) &gt; 0: bsz = inputs[0].size(self.dim) elif kwargs: bsz = list(kwargs.values())[0].size(self.dim) else: raise ValueError(\"You must pass inputs to the model!\") num_dev = len(self.device_ids) gpu0_bsz = self.gpu0_bsz # 除第一块之外每块GPU的bsz bsz_unit = (bsz - gpu0_bsz) // (num_dev - 1) if gpu0_bsz &lt; bsz_unit: # adapt the chunk sizes chunk_sizes = [gpu0_bsz] + [bsz_unit] * (num_dev - 1) delta = bsz - sum(chunk_sizes) # 补足偏移量 # 会有显存溢出的风险，因而最好给定的bsz是可以整除的 # e.g., 总的=52 =&gt; bsz_0=16, bsz_1=bsz_2=18 # 总的=53 =&gt; bsz_0=16, bsz_1=19, bsz_2=18 for i in range(delta): chunk_sizes[i + 1] += 1 if gpu0_bsz == 0: chunk_sizes = chunk_sizes[1:] else: return super().scatter(inputs, kwargs, device_ids) return scatter_kwargs(inputs, kwargs, device_ids, chunk_sizes, dim=self.dim)优缺点 优点：便于操作，理解简单 缺点：GPU分配不均匀；每次更新完都得销毁线程（运行程序后会有一个进程，一个进程可以有很多个线程）重新复制模型，因而速度慢Distributed Data Parallel实现原理 与DataParallel不同的是，Distributed Data Parallel会开设多个进程而非线程，进程数 = GPU数，每个进程都可以独立进行训练，也就是说代码的所有部分都会被每个进程同步调用，如果你某个地方print张量，你会发现device的差异 sampler会将数据按照进程数切分，确保不同进程的数据不同 每个进程独立进行前向训练 每个进程利用Ring All-Reduce进行通信，将梯度信息进行聚合 每个进程同步更新模型参数，进行新一轮训练按进程切分如何确保数据不同呢？不妨看看DistributedSampler的源码# 判断数据集长度是否可以整除GPU数# 如果不能，选择舍弃还是补全，进而决定总数# If the dataset length is evenly divisible by # of replicas# then there is no need to drop any data, since the dataset # will be split equally.if (self.drop_last and \tlen(self.dataset) % self.num_replicas != 0):\t# num_replicas = num_gpus self.num_samples = math.ceil((len(self.dataset) - self.num_replicas) /self.num_replicas)else: self.num_samples = math.ceil(len(self.dataset) / self.num_replicas) self.total_size = self.num_samples * self.num_replicas# 根据是否shuffle来创建indicesif self.shuffle: # deterministically shuffle based on epoch and seed g = torch.Generator() g.manual_seed(self.seed + self.epoch) indices = torch.randperm(len(self.dataset), generator=g).tolist() else: indices = list(range(len(self.dataset))) if not self.drop_last: # add extra samples to make it evenly divisible padding_size = self.total_size - len(indices) if padding_size &lt;= len(indices): # 不够就按indices顺序加 # e.g., indices为[0, 1, 2, 3 ...]，而padding_size为4 # 加好之后的indices[..., 0, 1, 2, 3] indices += indices[:padding_size] else: indices += (indices * math.ceil(padding_size / len(indices)))[:padding_size]else: # remove tail of data to make it evenly divisible. indices = indices[:self.total_size]assert len(indices) == self.total_size# subsample# rank代表进程idindices = indices[self.rank:self.total_size:self.num_replicas]return iter(indices)Ring All-Reduce那么什么是Ring All-Reduce呢？又为啥可以降低通信成本呢？首先将每块GPU上的梯度拆分成四个部分，比如$g_0 = [a_0; b_0; c_0; d_0]$，如下图（此部分原理致谢下王老师，讲的很清晰7）：所有GPU的传播都是同步进行的，传播的规律有两条： 只与自己下一个位置的GPU进行通信，比如0 &gt; 1，3 &gt; 0 四个部分，哪块GPU上占的多，就由该块GPU往它下一个传，初始从主节点传播，即GPU0，你可以想象跟接力一样，a传b，b负责传给c第一次传播如下：那么结果就是：那么，按照谁多谁往下传的原则，此时应该是GPU1往GPU2传a0和a1，GPU2往GPU3传b1和b2，以此类推接下来再传播就会有GPU3 a的部分全有，GPU0上b的部分全有等，就再往下传再来几遍便可以使得每块GPU上都获得了来自其他GPU的梯度啦代码使用基础概念第一个是后端的选择，即数据传输协议，从下表可以看出8，当使用CPU时可以选择gloo而GPU则可以是nccl Backend gloo   mpi   nccl   Device CPU GPU CPU GPU CPU GPU send ✓ ✘ ✓ ? ✘ ✓ recv ✓ ✘ ✓ ? ✘ ✓ broadcast ✓ ✓ ✓ ? ✘ ✓ all_reduce ✓ ✓ ✓ ? ✘ ✓ reduce ✓ ✘ ✓ ? ✘ ✓ all_gather ✓ ✘ ✓ ? ✘ ✓ gather ✓ ✘ ✓ ? ✘ ✓ scatter ✓ ✘ ✓ ? ✘ ✘ reduce_scatter ✘ ✘ ✘ ✘ ✘ ✓ all_to_all ✘ ✘ ✓ ? ✘ ✓ barrier ✓ ✘ ✓ ? ✘ ✓ 接下来是一些参数的解释9： Arg Meaning group 一次发起的所有进程构成一个group，除非想更精细通信，创建new_group world_size 一个group中进程数目，即为GPU的数量 rank 进程id，主节点rank=0，其他的在0和world_size-1之间 local_rank 进程在本地节点/机器的id 举个例子，假如你有两台服务器（又被称为node），每台服务器有4张GPU，那么，world_size即为8，rank=[0, 1, 2, 3, 4, 5, 6, 7], 每个服务器上的进程的local_rank为[0, 1, 2, 3]然后是初始化方法的选择，有TCP和共享文件两种，一般指定rank=0为master节点TCP显而易见是通过网络进行传输，需要指定主节点的ip（可以为主节点实际IP，或者是localhost）和空闲的端口import torch.distributed as distdist.init_process_group(backend, init_method='tcp://ip:port', rank=rank, world_size=world_size)共享文件的话需要手动删除上次启动时残留的文件，加上官方有一堆警告，还是建议使用TCPdist.init_process_group(backend, init_method='file://Path', rank=rank, world_size=world_size)launch方法初始化这里先讲用launch的方法，关于torch.multiprocessing留到后面讲在启动后，rank和world_size都会自动被DDP写入环境中，可以提前准备好参数类，如argparse这种args.rank = int(os.environ['RANK'])args.world_size = int(os.environ['WORLD_SIZE'])args.local_rank = int(os.environ['LOCAL_RANK'])首先，在使用distributed包的任何其他函数之前，按照tcp方法进行初始化，需要注意的是需要手动指定一共可用的设备CUDA_VISIBLE_DEVICESdef dist_setup_launch(args): # tell DDP available devices [NECESSARY] os.environ['CUDA_VISIBLE_DEVICES'] = args.devices args.rank = int(os.environ['RANK']) args.world_size = int(os.environ['WORLD_SIZE']) args.local_rank = int(os.environ['LOCAL_RANK']) dist.init_process_group(args.backend, args.init_method, rank=args.rank, world_size=args.world_size) # this is optional, otherwise you may need to specify the # device when you move something e.g., model.cuda(1) # or model.to(args.rank) # Setting device makes things easy: model.cuda() torch.cuda.set_device(args.rank) print('The Current Rank is %d | The Total Ranks are %d' %(args.rank, args.world_size))DistributedSampler接下来创建DistributedSampler，是否pin_memory，根据你本机的内存决定。pin_memory的意思是提前在内存中申请一部分专门存放Tensor。假如说你内存比较小，就会跟虚拟内存，即硬盘进行交换，这样转义到GPU上会比内存直接到GPU耗时。因而，如果你的内存比较大，可以设置为True；然而，如果开了导致卡顿的情况，建议关闭from torch.utils.data import DataLoader, DistributedSamplertrain_sampler = DistributedSampler(train_dataset, seed=args.seed)train_dataloader = DataLoader(train_dataset, pin_memory=True, shuffle=(train_sampler is None), batch_size=args.per_gpu_train_bs, num_workers=args.num_workers, sampler=train_sampler)eval_sampler = DistributedSampler(eval_dataset, seed=args.seed)eval_dataloader = DataLoader(eval_dataset, pin_memory=True, batch_size=args.per_gpu_eval_bs, num_workers=args.num_workers, sampler=eval_sampler)加载模型然后加载模型，跟DataParallel不同的是需要提前放置到cuda上，还记得上面关于设置cuda_device的语句嘛，因为设置好之后每个进程只能看见一个GPU，所以直接model.cuda()，不需要指定device同时，我们必须给DDP提示目前是哪个rankfrom torch.nn.parallel import DistributedDataParallel as DDPmodel = model.cuda()# tell DDP which rankmodel = DDP(model, find_unused_parameters=True, device_ids=[rank])注意，当模型带有Batch Norm时：if args.syncBN: nn.SyncBatchNorm.convert_sync_batchnorm(model).cuda()训练相关每个epoch开始训练的时候，记得用sampler的set_epoch，这样使得每个epoch打乱顺序是不一致的关于梯度回传和参数更新，跟正常情况无异for epoch in range(epochs): # record epochs train_dataloader.sampler.set_epoch(epoch) outputs = model(inputs) loss = loss_fct(outputs, labels) loss.backward() optimizer.step() optimizer.zero_grad()这里有一点需要小心，这个loss是各个进程的loss之和，如果想要存储每个step平均损失，可以进行all_reduce操作，进行平均，不妨看官方的小例子来理解下：&gt;&gt;&gt; # All tensors below are of torch.int64 type.&gt;&gt;&gt; # We have 2 process groups, 2 ranks.&gt;&gt;&gt; tensor = torch.arange(2, dtype=torch.int64) + 1 + 2 * rank&gt;&gt;&gt; tensortensor([1, 2]) # Rank 0tensor([3, 4]) # Rank 1&gt;&gt;&gt; dist.all_reduce(tensor, op=ReduceOp.SUM)&gt;&gt;&gt; tensortensor([4, 6]) # Rank 0tensor([4, 6]) # Rank 1@torch.no_grad()def reduce_value(value, average=True): world_size = get_world_size() if world_size &lt; 2: # 单GPU的情况 return value dist.all_reduce(value) if average:\t value /= world_size return value看到这，肯定有小伙伴要问，那这样我们是不是得先求平均损失再回传梯度啊，不用，因为，当我们回传loss后，DDP会自动对所有梯度进行平均10，也就是说回传后我们更新的梯度和DP或者单卡同样batch训练都是一致的loss = loss_fct(...)loss.backward()# 注意在backward后面loss = reduce_value(loss, world_size)mean_loss = (step * mean_loss + loss.item()) / (step + 1)还有个注意点就是学习率的变化，这个是和batch size息息相关的，如果batch扩充了几倍，也就是说step比之前少了很多，还采用同一个学习率，肯定会出问题的，这里，我们进行线性增大11N = world_sizelr = args.lr * N肯定有人说，诶，你线性增大肯定不能保证梯度的variance一致了，正确的应该是正比于$\\sqrt{N}$，关于这个的讨论不妨参考12evaluate相关接下来，细心的同学肯定好奇了，如果验证集也切分了，metric怎么计算呢？此时就需要咱们把每个进程得到的预测情况集合起来，t就是一个我们需要gather的张量，最后将每个进程中的t按照第一维度拼接，先看官方小例子来理解all_gather&gt;&gt;&gt; # All tensors below are of torch.int64 dtype.&gt;&gt;&gt; # We have 2 process groups, 2 ranks.&gt;&gt;&gt; tensor_list = [torch.zeros(2, dtype=torch.int64) for _ in range(2)]&gt;&gt;&gt; tensor_list[tensor([0, 0]), tensor([0, 0])] # Rank 0 and 1&gt;&gt;&gt; tensor = torch.arange(2, dtype=torch.int64) + 1 + 2 * rank&gt;&gt;&gt; tensortensor([1, 2]) # Rank 0tensor([3, 4]) # Rank 1&gt;&gt;&gt; dist.all_gather(tensor_list, tensor)&gt;&gt;&gt; tensor_list[tensor([1, 2]), tensor([3, 4])] # Rank 0[tensor([1, 2]), tensor([3, 4])] # Rank 1def sync_across_gpus(t, world_size): gather_t_tensor = [torch.zeros_like(t) for _ in range(world_size)] dist.all_gather(gather_t_tensor, t) return torch.cat(gather_t_tensor, dim=0)可以简单参考我前面提供的源码的evaluate部分，我们首先将预测和标签比对，把结果为bool的张量存储下来，最终gather求和取平均。这里还有个有趣的地方，tensor默认的类型可能是int，bool型的res拼接后自动转为0和1了，另外bool型的张量是不支持gather的def eval(...) results = torch.tensor([]).cuda() for step, (inputs, labels) in enumerate(dataloader): outputs = model(inputs) res = (outputs.argmax(-1) == labels) results = torch.cat([results, res], dim=0) results = sync_across_gpus(results, world_size) mean_acc = (results.sum() / len(results)).item() return mean_acc模型保存与加载模型保存，参考部分官方教程13，我们只需要在主进程保存模型即可，注意，这里是被DDP包裹后的，DDP并没有state_dict，这里barrier的目的是为了让其他进程等待主进程保存模型，以防不同步def save_checkpoint(rank, model, path): if is_main_process(rank): \t# All processes should see same parameters as they all # start from same random parameters and gradients are # synchronized in backward passes. # Therefore, saving it in one process is sufficient. torch.save(model.module.state_dict(), path) # Use a barrier() to keep process 1 waiting for process 0 dist.barrier()加载的时候别忘了map_location，我们一开始会保存模型至主进程，这样就会导致cuda:0显存被占据，我们需要将模型remap到其他设备def load_checkpoint(rank, model, path): # remap the model from cuda:0 to other devices map_location = {'cuda:%d' % 0: 'cuda:%d' % rank} model.module.load_state_dict( torch.load(path, map_location=map_location) )进程销毁运行结束后记得销毁进程：def cleanup(): dist.destroy_process_group() cleanup()如何启动在终端输入下列命令【单机多卡】python -m torch.distributed.launch --nproc_per_node=NUM_GPUS main.py (--arg1 --arg2 --arg3 and all other arguments of your training script)目前torch 1.10以后更推荐用runtorch.distributed.launch -&gt; torch.distributed.run / torchrun多机多卡是这样的：# 第一个节点启动python -m torch.distributed.launch \\ --nproc_per_node=NUM_GPUS \\ --nnodes=2 \\ --node_rank=0 \\ --master_addr=\"192.168.1.1\" \\ --master_port=1234 main.py # 第二个节点启动python -m torch.distributed.launch \\ --nproc_per_node=NUM_GPUS \\ --nnodes=2 \\ --node_rank=1 \\ --master_addr=\"192.168.1.1\" \\ --master_port=1234 main.py mp方法第二个方法就是利用torch的多线程包import torch.multiprocessing as mp# rank mp会自动填入def main(rank, arg1, ...): passif __name__ == '__main__': mp.spawn(main, nprocs=TOTAL_GPUS, args=(arg1, ...))这种运行的时候就跟正常的python文件一致：python main.py优缺点 优点： 相比于DP而言，不需要反复创建和销毁线程；Ring-AllReduce算法提高通信效率；模型同步方便 缺点：操作起来可能有些复杂，一般可满足需求的可先试试看DataParallelReferences https://blog.csdn.net/qq_37541097/article/details/109736159 &#8617; https://www.cnblogs.com/ljwgis/p/15471530.html &#8617; https://pytorch.org/tutorials/beginner/former_torchies/parallelism_tutorial.html?highlight=dataparallel &#8617; https://pytorch.org/tutorials/beginner/blitz/data_parallel_tutorial.html &#8617; https://github.com/kimiyoung/transformer-xl &#8617; https://github.com/sherlcok314159/dl-tools/blob/main/balanced_data_parallel/README.md &#8617; https://www.youtube.com/watch?v=rj-hjS5L8Bw &#8617; https://pytorch.org/docs/stable/distributed.html#backends &#8617; https://stackoverflow.com/questions/58271635/in-distributed-computing-what-are-world-size-and-rank &#8617; https://discuss.pytorch.org/t/average-loss-in-dp-and-ddp/93306/4 &#8617; https://arxiv.org/abs/1706.02677 &#8617; https://github.com/Lightning-AI/lightning/discussions/3706 &#8617; https://pytorch.org/tutorials/intermediate/ddp_tutorial.html &#8617; " }, { "title": "Going Deeper into Back-propagation", "url": "/posts/Back-propagation/", "categories": "Training, Deep Learning", "tags": "Gradients, Optimization", "date": "2022-09-07 11:10:00 +0800", "snippet": "1. Gradient descent optimizationGradient-based methods make use of the gradient information to adjust the parameters. Among them, gradient descent can be the simplest. Gradient descent makes the parameters to walk a small step in the direction of the negative gradient.\\[\\boldsymbol{w}^{\\tau + 1} = \\boldsymbol{w}^{\\tau} - \\eta \\nabla_{\\boldsymbol{w}^{\\tau}} E \\tag{1.1}\\]where $\\eta, \\tau, E$ label learning rate ($\\eta &gt; 0$), the iteration step and the loss function.Wait! But why is the negative gradient?2. Why negative gradient The function increases the most sharply by following the direction of the gradient.The below is an example. The three-dimensional plane is $z = F(x, y)$. The black point is on the plane. You can try to move the point to see how the arrow changes. Interestingly, the arrow always points to the direction which leads to the biggest increase of the function value. Note that when you move one step, the gradient just changes. Thus if you still want to increase the function value in the most sharp way, another computation is needed. The starting point of the arrow is the mapping of the black point to the $xoy$ plane. The arrow is parallel to the gradient. Let us use another graph to better understand what the mapping means. The left graph is contour plot while the right is the plane. The red point is just the mapping of the black point to $xoy$ plane. The blue arrow is just the direction of the gradient. And you can move the point to feel about it. That is the intuitive way to feel about the gradient. Furthermore, we can just try to prove it1.Consider a Taylor Expansion:\\[\\begin{aligned}F(\\boldsymbol{r_0} + \\boldsymbol{r}) &amp;= F(\\boldsymbol{r_0}) + \\nabla_{r_0}F \\cdot \\boldsymbol{r}\\\\&amp;= F(\\boldsymbol{r_0}) + \\|\\nabla_{r_0}F\\|\\cdot \\|\\boldsymbol{r}\\| \\cdot \\cos \\theta \\end{aligned}\\tag{2.1}\\]When you decide to move a small step, the two magnitudes are certain. If $\\theta=0$, you can maximize the function value (i.e. in the direction of the gradient).Thus if we want to minimize our loss function, we need to go in the opposite direction of the gradient. That is why we need a negative gradient. Also, note that Taylor Expansion only applies to small $\\Delta x$ which further requires $\\eta$ to be small (e.g. $2 \\times 10^{-5}, 5 \\times 10^{-5}$).But how to compute the gradient needs a powerful technique: back-propagation.3. Definition of back-propagation Back-propagation allows information from the cost to then flow backwards through the network, in order to compute the gradients used to adjust the parameters.Back-propagation can be new to the novices, but it does exist in the life widely. For instance, the loss can be your teacher’s attitude towards you. If you fail in one examination, your teacher can be disappointed with you. Then, he can tell your parents about your failure. Your parents then ask you to work harder to win the examination.Your parents can be seen as hidden units in the neural network, and you are the parameter of the network. Your teacher’s bad attitude towards your failure can ask you to make adjustments: working harder. Similarly, the loss can require the parameters to make adjustments via gradients.4. Chain RuleSuppose $z = f(y), y = g(x) \\implies z = (f \\circ g)(x)$, how to calculate the derivative of $z$ with respect to $x$? The chain rule of calculus is used to compute the derivatives of functions formed by composing other functions whose derivatives are known.\\[\\frac{dz}{dx} = \\frac{dz}{dy} \\frac{dy}{dx} \\tag{4.1}\\]5. Case StudyLet’s first see an important example. In fully connected layers, one input neuron sends information (i.e., multiplied by weights) to every output neuron. Denote $w_{ji}$ as the weight from $x_i$ to $y_j$. Then for every output neuron (e.g., $y_j$), it accepts the information sent by every input neuron:\\[y_{j}= \\sum\\limits_{i} w_{ji} x_{i} \\tag{5.1}\\]Then the partial derivative of $y_j$ with respect to $x_i$:\\[\\frac{\\partial y_j}{\\partial x_{i}}= w_{ji} \\tag{5.2}\\]Let’s see another example2. It is represented by the computational graph3 below.And we can perform a forward propagation according to the computational graph.\\[\\begin{align}h_{j} &amp;= \\sum\\limits_{i} w_{ji}^{(1)} x_{i} \\tag{5.3} \\\\z_{j} &amp;= f(h_{j}) \\tag{5.4} \\\\y_{k} &amp;= \\sum\\limits_{j}w_{kj}^{(2)} z_{j} \\tag{5.5} \\end{align}\\]where\\[f(h) = \\tanh(h) = \\frac{e^h - e^{-h}}{e^h + e^{-h}} \\tag{5.6}\\]A useful feature of this activation is that its derivative can be expressed in a particularly simple form:\\[f'(h) = 1 - f(h)^2 \\tag{5.7}\\]The error function can be mean squared errors:\\[E(\\boldsymbol{w}) = \\frac{1}{2} \\sum\\limits_{k}(y_{k}- \\hat{y}_k)^2 \\tag{5.8}\\]If we want to update the parameters, we need first to compute the partial derivative of $E(\\boldsymbol{w})$ with respect to them.\\[\\frac{\\partial E(\\boldsymbol{w})}{\\partial w_{kj}^{(2)}} = \\frac{\\partial E(\\boldsymbol{w})}{\\partial y_{k}} \\frac{\\partial y_k}{\\partial w_{kj}^{(2)}} = (y_{k}- \\hat{y}_k)z_{j} \\tag{5.9}\\]\\[\\begin{align}\\frac{\\partial E(\\boldsymbol{w})}{\\partial w_{ji}^{(1)}} &amp;= \\frac{\\partial E(\\boldsymbol{w})}{\\partial h_{j}}\\frac{\\partial h_j}{\\partial w_{ji}^{(1)}} = (\\frac{\\partial E(\\boldsymbol{w})}{\\partial z_{j}} \\frac{\\partial z_j}{\\partial h_j})x_{i} \\tag{5.10} \\\\\\end{align}\\]\\[\\frac{\\partial E(\\boldsymbol{w})}{\\partial z_j} = \\sum\\limits_{k}\\frac{\\partial E(\\boldsymbol{w})}{\\partial y_{k}}\\frac{\\partial y_k}{\\partial z_{j}}= \\sum\\limits_{k} (y_{k}- \\hat{y}_{k}) w_{kj}^{(2)}\\tag{5.11}\\]$\\text{Remark.}$ $z_j$ can send information to all the output neurons (e.g., $y_k$), thus we need to sum over all the derivatives with respect to $z_j$.Substituting $\\text{(4.11)}$ into $\\text{(4.10)}$ we obtain\\[\\frac{\\partial E(\\boldsymbol{w})}{\\partial w_{ji}^{(1)}} = (1 - z_j^2)x_{i} \\sum\\limits_{k} (y_{k}- \\hat{y}_{k}) w_{kj}^{(2)} \\tag{5.12}\\]6. InterpretationRecall the Taylor approximation of the two variables function:\\[f(x, y) = f(x_0, y_0) + f_x (x- x_0) + f_y(y-y_0) \\tag{6.1}\\]$\\text{Remark.}$ $(x, y)$ needs to be close to $(x_0, y_0)$, otherwise the approximation can fail.We can transform $\\text{(5.1)}$ into $\\text{(5.3)}$:\\[\\begin{align}f(x,y) - f(x_{0},y_0) &amp;= f_x (x- x_0) + f_y(y-y_0) \\tag{6.2}\\\\\\implies \\Delta f &amp;= f_x \\Delta x + f_y \\Delta y\\tag{6.3}\\end{align}\\]If we apply $\\text{(5.3)}$ in the example above, we can obtain\\[\\Delta E(\\boldsymbol{w}) = \\nabla_{\\boldsymbol{w}}E(\\boldsymbol{w}) \\Delta \\boldsymbol{w} \\tag{6.4}\\]From another perspective, a small change in the parameters will propagate into a small change in object function by getting multiplied by the gradient. To summarize, back-propagation allows information to flow backwards through the network. This information can tell the model a small change in one particular parameter can result in what change in the object function. And gradient descent can use this information to adjust the parameters for optimizing the object function.7. References https://math.stackexchange.com/questions/223252/why-is-gradient-the-direction-of-steepest-ascent &#8617; Bishop-Pattern-Recognition-and-Machine-Learning-2006 &#8617; Ian Goodfellow, Yoshua Bengio, Aaron Courville - Deep Learning [draft of March 30, 2015] (2016, MIT Press) &#8617; " }, { "title": "Tips for Training Neural Networks", "url": "/posts/Tips-for-Training-Neural-Networks/", "categories": "Training, Deep Learning", "tags": "Experience, Neural Networks", "date": "2022-07-30 19:43:00 +0800", "snippet": "Recently, I have read a blog about training neural networks (simplified as NN in the rest part of this post) and it is really amazing. I am going to add my own experience in this post along with summarizing that blog’s interesting part.Nowadays, it seems like that training NN is extremely easy for there are plenty of free frameworks which are simple to use (e.g. PyTorch, Numpy, Tensorflow). Well, training NN is easy when you are copying others’ work (e.g. reproducing a BERT) because everything is there for you. However, when designing a NN or facing a new task, you are most probably trapped somewhere.And this blog is meant to guide you to handle new problems.Let’s first begin with some basic rules. Hope you guys enjoy it! Rush into training neural networks leads to suffering. Training NN is not like writing the common code. For instance, if you plug in a int while it needs a string, errors just come out. However, writing the code about NN can not be so easy for it won’t show you bugs automatically (only if you make big mistakes). Sweating the details always pays off. Someone may say the details are infinite and can stop you from marching. Note that the details mentioned here are all necessary instead of some trivialities. And sweating the details can reduce your pain. Observation leads to intuition. Sadly, if you just keep thinking about something, inspiration will never come to you. For instance, if you want to upgrade the algorithm, you had better observe the data where the algorithm fails instead of just thinking about the algorithm. Trusting your intuition instead of your implementation. Sometimes when you come up with a new idea, the implementation of it may go wrong to some degree. When the result is opposite to your assumption, always check your code first before doubting your idea. Quicker, then better. When you are trying to test your hypothesis, use the most efficient way to verify it as fast as possible.Then let us go through concrete parts.1. Familiar with DataAt this stage, you need to do some basic analysis and data mining. Assume we have a classification dataset in NLP. There are several aspects to think about. The Distribution. To begin with, you need to know the label distribution especially and visualize it. For instance, if you observe long-tailed distribution (e.g. the number of the instances for good emotion is 900 while for the bad emotion is 10), then some methods such as data augmentation or cost-sensitive loss functions can play their part. For your interest, you can refer to this up-to-date survey. Similarly, you may also need to know the distribution of the length of sequence. Thus you can set the appropriate maximum sequence length for your task. Moreover, you can also pass the original data through the feature extractor (such as BERT) to gain their representation. Then you can cluster them. Greed is good. I strongly suggest that you look through the whole dataset ambitiously just like the greedy algorithm. And I promise you are bound to find surprise. You can have a whole understanding of the domain of this dataset. And you can choose appropriate pre-trained models according to the domain. Also, remember to understand the labels. Once you understand the labels, you can see if the annotation is contradictory. And you can select certain samples to see the annotation and estimate how noisy the whole dataset is. You may also need to think for the machine. Are the sequences easy to understand? If they are easy to understand, then we do not need to apply very complicated models to tackle this problem. Is the local information more important than the global? Your understanding about the dataset can help you figure out some basic modeling problems and offer you intuition about rule-based methods. Simple quantification. You may need to know the size of the dataset. If the size is small, we can use the simple models such as textCNN or FastText instead of BERT-based models for the complicated models need more data to model the inductive bias. Also, you can write simple code for detecting the duplicate instances and instances which are corrupted (e.g. lack of the label). Stand on the shoulder of the model. When your model is trained on the dataset, you can see how it performs on the dev/eval set. You need to pay attention to those mis-predicted instances (i.e. bad cases) and think about why the prediction is wrong. Is the label wrong or the way of modeling weak to capture these information. Filter / Sort / Clean. You can decide whether to filter or clean the dataset based on your thorough observation.2. End-to-End PipelineWhen you finish observing the dataset, you need to build the simple pipeline to ensure everything goes well. Fix the random seed. When carrying out the experiments, you had better fix the seed to reduce the influences of randomness on the experiments. As simple as possible. When building the pipeline, you do not have to use very complicated modeling methods, etc. We are just testing. Thus make everything as simple as possible. For instance, you can use the simple classifier such as SVM and MLP (Multi-Layer Perceptron). Record the accuracy and loss. Training accuracy and loss are very useful for you to figure out which difference is beneficial. Also, we do not need complicated tools (e.g. Tensorboard and Wandb) to do so. You can use a list to store things you want and visualize it by matplotlib or write it down in a txt (Sometimes, the data on the terminal can disappear for certain reasons). Track the progress. In python, you can simply use the tqdm to track the progress. And you can also add the immediate accuracy and loss on the progress bar. Believe me, this can reduce your anxiety. Verify the init loss. For the multi-label classification problem, its loss should equal $-\\log (1/ \\text{num classes})$ (with a softmax). For instance, if you need to make the true prediction among 4 labels, the init loss should equal $1.386$. Good Initialization. For regression problems, if the average of your data is 6, then you can initialize the bias as 6 which can lead to fast and stable convergence. One more example, if you want to initialize the weights and you do not want the weights to be influenced by the output shape, then you may prefer Lecun Normal to Glorot Normal (all initialize with $\\text{N(0, scale)}$). Also, normal initialization is better than uniform initialization by experience. Last but not least, when facing an imblanced dataset with ratio 1:10 (positive cases V.S. negative cases), set the bias on the logits so that the model can learn the bias with the first few iterations. # fan_in, fan_out represent the input and output shape scale = 1. # lecun normal scale_lecun /= max(1., fan_in) scale_lecun = sqrt(scale_lecun)\t # glorot normal scale_glorot /= max(1., fan_in + fan_out) scale_glorot = sqrt(scale_glorot) Human Baseline. If the dataset is very particular and there are few related evaluation methods, you had better set the human baseline in sampled instances. Compared to the human baseline, you can have an idea that where your model has gone. Input-Independent Baseline. You can set the input all zeros and see the performance. And it should be worse than the performance of plugging in your data. Overfit a small batch. The model should overfit a batch of few instances (e.g. 10 or 20). Theoretically speaking, you should achieve the least loss. If the model fails to do so, then you can go and find the foxy bug. Visualize the input before going into the NN. Take Google’s code as example, it shows the input tensors when performing classification problems by BERT. This habit has saved me many times when coming up with a brand-new task because the preparation of data can be hard to some degree. Visualize the predictions dynamically over the course of training. By doing so, we can have a direct picture about where the model has gone and how it performs. Try Back-Propogation yourself. Gradients can give you information about what the model depends on to make such predictions. Generalize a special case. You should not write the general functions at the beginning because your thoughts can be easy to change, thus these general functions are fragile. You can generalize a special case when you are sure that it won’t change a lot.3. OverfittingSince we have built a pipeline and tested it, it’s time for us to make the model overfit the whole dataset. Picking the right model. The selection of models is related to the size of the dataset and complexity of the task. If your dataset is small, you can choose relatively big models to overfit. Borrow experience from the giants. Sometimes you are unfamiliar with the task, you may have no idea about which hyper-parameter to choose (e.g. learning rate). Then you can search some related papers and see the appendix for training details. Carry out many controlled experiments. Deep Learning is a experimental subject. Sometimes observation fails to give you idea about what exactly it will perform. For instance, if you want to know which learning rate is most suitable for this task, try more options to select the best. Remember change a variable once a time to reduce the influence of mixture. Turn to tricks. Tricks are infinite. For instance, you can apply adversarial training like FGM or PGD to improve the model’s performance. Also, if permitted, you can use random searching for the best parameters.4. Regularize More data is better. The most effective way to regularize the model is collecting more real-world data for training. After all, we are using the small set of data to approximate the distribution of the real-world. Data Augmentation. If you lack data, you can apply data augmentation to increase your data. Although this method seems very easy, it does demand your thorough understanding of your data and task. And creative methods can always pay off. For instance, in NLP, you can use back-translation to augment. Cross Validation. You can split the data several times. And use separate data to train some models. Then we can ensemble them to gain the final prediction.5. Others Always remember to record your results in a good order. For instance, you must record all the parameters and the model’s performances at the dev/eval set. You had better record the motivation for you trying out this experiment. Always back up your code and data. When you are trying some new methods, do not just try it on the original code. The same for the data. You need to back up the original pipeline and data for bad things happening." }, { "title": "Quotes of Mathematicians", "url": "/posts/Quotes-of-Mathematicians/", "categories": "Quotes", "tags": "Mathematics", "date": "2022-07-23 09:56:00 +0800", "snippet": " Life is complex, and it has both real and imaginary parts. — Someone Basically, I’m not interested in doing research and I never have been… I’m interested in understanding, which is quite a different thing. And often to understand something you have to work it out yourself because no one else has done it. — David Blackwell To not know maths is a severe limitation to understanding the world. — Richard Feynman Problems worthy of attack prove their worth by fighting back. — Piet Hein An expert is a person who has made all the mistakes that can be made in a very narrow field. — Niels Bohr The beauty of mathematics is only shown to its patient followers. — Maryam Mirzakhani The essence of Mathematics lies precisely in its freedom. — Georg Cantor We must know, we will know. — David Hilbert If your tendency is to make sense out of chaos, start chaos. — Carlos Castaneda Perhaps I could best describe my experience of doing mathematics in terms of entering a dark mansion. You go into the first room and it’s dark, completely dark. You stumble around, bumping into the furniture. Gradually, you learn where each piece of furniture. And finally, after six months or so, you find the light switch and turn it on. Suddenly, it’s all illuminated and you can see exactly where you were. Then you enter the next dark room… — Andrew Wiles If we knew what it is we were doing, it would not be called research. Would it? — Albert Einstein If you can’t solve a problem, then there is an easier problem you can solve: find it. — George Pólya Young man, in mathematics you don’t understand things. You just get used to them. — John Von Neumann Mathematics is the art of giving the same name to different things. — Henri Poincare Mathematics is the subject in which we never know what we’re talking about, nor whether what we are saying is true. — Bertrand Russell All mathematics models are wrong. Some of them are useful. — George Box Whenever something goes wrong, remember 1+1 is still 2. — Someone The study of mathematics is apt to commence in disappointment… We are told that by its aid the stars are weighed and the billions of molecules in a drop of water are counted. Yet, like the ghost of Hamlet’s father, this great science eludes the efforts of our mental weapons to grasp it. — A. N. Whitehead When one does a theoretical calculation, there are two ways of doing it. Either one should have a clear physical model in mind or a rigorous mathematical basis. — Enrico Fermi Let the formula guide your thinking. — Someone A mathematician is a device for turning coffee into theorems. — Alfréd Rényi As long as there’s a life, there’s hope. — Stephen Hawking Every problem was once “unsolvable” until it was solved. Let us assume we know nothing, which is a reasonable approximation. — D. Kazhdan Failure is not an option. Failure is mandatory. The option is whether or not you let failure be the last thing you do. — Howard Tayler Mathematics is not about numbers, equations, computations, or algorithms: it is about understanding. — William Paul Thurston What is mathematics? It is only a systematic effort of solving puzzles posed by nature. — Shakuntala Devi Doing mathematics should always mean finding patterns and crafting beautiful and meaningful explanations. — Paul Lockhart Obvious is the most dangerous word in mathematics. — Eric Temple Bell Mathematics is the poetry of logical ideas. — Albert Einstein Only one thing can comfort your feelings like music, please your heart like paintings, stir your soul like poetry, enhance your wisdom like philosophy, and improve your life like technology — Mathematics. — Klein" }, { "title": "Retrieval-Enhanced Transformer", "url": "/posts/Retieval-Enhanced-transformer/", "categories": "PLMs", "tags": "retrieval, transformer", "date": "2022-06-19 21:06:00 +0800", "snippet": "Problems To Solve To Scale Down the model size while maintaining the performances. To incorporate External Memory Retrieval in the Large Language Model Modeling.How?Data Construction Training &amp; Evaluation set: $\\text{MassiveText}$ for both training &amp; retrieval data (contains 5 trillion tokens) SentencePiece with a vocabulary of $128K$ tokens During training, we retrieving $600B$ tokens from the training The evaluation contains $1.75T$ tokens Test set leakage: Due to the huge retrieving database, the test set may have appeared in the training set. Thus, the authors apply 13-gram Jaccard Similarity between the training and test documents to filter those training documents similar to the test documents (i.e., the similarity is $\\geq \\textbf{0.80}$)Retrieval Modeling Key-Value Format of the Database: $\\text{Key} \\Rightarrow$ frozen BERT Embedding $\\text{Value} \\Rightarrow$ raw chunks of the tokens using the SCaNN library the similarity depends on the $\\text{L2 Distance}$:\\[||x-y||_2 = \\sqrt{\\sum_i (x_i - y_i)^2}\\] pre-compute the frozen BERT Embedding to save the computation and the Embedding is averaged with time. retrieving targets are the corresponding chunks and their continuation in the orig documentThe whole architectureThe pipeline Assume the input sequence $\\text{X}$ contains $9$ tokens, it can be split into $3$ chunks (i.e., $C_1, C_2, C_3$) whose sizes are $3$ respectively. Then the chunks are embedded through the frozen BERT embedding. We can retrieve neighbours of those input chunks. We also embed the input sequence and then apply self-attention mechanism on them to get the hidden states $H(X)$ Furthermore, we need to encode the neighbours. Here, the transformer encoder is bi-directional. And it outputs the representations of the neighbours by conditioning on the hidden states of the input chunks. After we get the representations of the neighbours, we let them attend the input chunks as the $\\text{K and V}$ while the input chunk is $\\text{Q}$. The attending network is called CCA($\\textbf{C}$hunked $\\textbf{C}$ross $\\textbf{A}$ttention). I introduce it in the following part. When the neighbours finish attending the input chunks, the input chunks can be represented by the retrieved neighbours. The representations are going through the FFW($\\textbf{F}$eed $\\textbf{F}$or$\\textbf{W}$ard). Thus, a Retro-Block contains self-attention mechanism, CCA and FFW.Chunked Cross Attention Take the green chunk as the example, we retrieve its neighbours from the database and we let them attend with the concatenation between the green chunk and its next chunk. To put it more precisely, assume we retrieve the neighbours $E(m_i)$ for the chunk $m_i$ which contains $n$ tokens: ${m_{i1}, m_{i2}, \\dots, m_{in}}$, we concatenate the last token of $m_i$ with the next chunk $m_j$ except the last token $\\Rightarrow \\text{Concatenate}(m_{in}, m_{j1, \\dots, jn-1})$. After the concatenation, we apply CA($\\textbf{C}$ross $\\textbf{A}$ttention). CA is the common attention mechanism. Finally, we concatenate the outputs and pad them. Note, the relative positional encoding is applied.ExperimentScaling the Retro The scale of the Retro and the retrieved tokens are proportional to the performance. The number of neighbours has an upped bound: somewhere near $40$. Maybe too many neighbours reduce the retrieval quality.Improvement Comparison Among some tasks, Retro can outperform the models whose parameters are much more than the Retro’s.Perplexity on Wikitext103 Retro’s perplexity can be SOTA on the Wikitext103 Interestingly, the external memory can also have the phenomenon of the underfitting. When using MassiveText(1%), it can underfit the training set. And its performance is worse than the kNN-LM.Retro Finetuning Training from scratch is the most powerful way.Question Answering Results FID + Distill is the SOTA in the Open-Domain Question Answering when the retrieval involves in the training.Ablation Studies The continuation of the retrieved chunks do help. CA positions are every 3 from 1 or mid layer.Why work? To summarize, the Retro incorporates the external neighbours of the input sequence into the Large Language Modelling to scale down the model size while maintaining the performance.Lessons &amp; Imaginations Performance can get improved either by improving the model size or training more data. Huge amount of data don't need too big model to fit in. We can scale down the PLM by attending the external information. CCA is applied because the external knowledge need to be merged. When applying in MRC, the external information can be: the chunked passages the broken passages the past similar to question-passage pairs the knowledge among the input the evidence The BM25, Edit Distance and LDA can also perform not bad in the retieval." }, { "title": "Hello World", "url": "/posts/hello-world/", "categories": "Misc", "tags": "1st post, hello world", "date": "2022-06-19 11:23:00 +0800", "snippet": "Hello! You are reading my first post!In the following days, I am going to post the thoughts about Mathematics/ML/NLP. And the updating routine is at least one post per week. There can be interesting visualization in every post to help you understand some complex concept easily.Last but not least, acknowledge and credits should be given to Hummat who provided the complete resources for building the 3D website, Cotes who provided the theme and Barry Clark who built the jekyll-now.Kind Regards,Yunpeng Tai" } ]
