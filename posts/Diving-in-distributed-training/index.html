<!DOCTYPE html><html lang="en" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="generator" content="Jekyll v4.2.2" /><meta property="og:title" content="Diving in distributed training in PyTorch" /><meta property="og:locale" content="en" /><meta name="description" content="鉴于网上此类教程有不少模糊不清，对原理不得其法，代码也难跑通，故而花了几天细究了一下相关原理和实现，欢迎批评指正！" /><meta property="og:description" content="鉴于网上此类教程有不少模糊不清，对原理不得其法，代码也难跑通，故而花了几天细究了一下相关原理和实现，欢迎批评指正！" /><link rel="canonical" href="https://www.yunpengtai.top/posts/Diving-in-distributed-training/" /><meta property="og:url" content="https://www.yunpengtai.top/posts/Diving-in-distributed-training/" /><meta property="og:site_name" content="Yunpeng Tai" /><meta property="og:image" content="https://s2.loli.net//2022/11/20/xKhQypW5SeY4oEt.jpg" /><meta property="og:image:height" content="100" /><meta property="og:image:width" content="600" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2022-11-20T21:37:00+08:00" /><meta name="twitter:card" content="summary_large_image" /><meta property="twitter:image" content="https://s2.loli.net//2022/11/20/xKhQypW5SeY4oEt.jpg" /><meta property="twitter:title" content="Diving in distributed training in PyTorch" /><meta name="twitter:site" content="@TonySta14611077" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2022-11-22T09:23:15+08:00","datePublished":"2022-11-20T21:37:00+08:00","description":"鉴于网上此类教程有不少模糊不清，对原理不得其法，代码也难跑通，故而花了几天细究了一下相关原理和实现，欢迎批评指正！","headline":"Diving in distributed training in PyTorch","image":{"width":600,"height":100,"url":"https://www.yunpengtai.top/2022/11/20/xKhQypW5SeY4oEt.jpg","@type":"imageObject"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://www.yunpengtai.top/posts/Diving-in-distributed-training/"},"url":"https://www.yunpengtai.top/posts/Diving-in-distributed-training/"}</script><title>Diving in distributed training in PyTorch | Yunpeng Tai</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="Yunpeng Tai"><meta name="application-name" content="Yunpeng Tai"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin><link rel="dns-prefetch" href="https://fonts.gstatic.com" crossorigin><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://cdn.jsdelivr.net" ><link rel="dns-prefetch" href="https://cdn.jsdelivr.net" ><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Lato&family=Source+Sans+Pro:wght@400;600;700;900&display=swap"><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://www.google-analytics.com"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://www.googletagmanager.com"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1/dist/magnific-popup.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script> <script type="text/javascript"> class ModeToggle { static get MODE_KEY() { return "mode"; } static get MODE_ATTR() { return "data-mode"; } static get DARK_MODE() { return "dark"; } static get LIGHT_MODE() { return "light"; } static get ID() { return "mode-toggle"; } constructor() { if (this.hasMode) { if (this.isDarkMode) { if (!this.isSysDarkPrefer) { this.setDark(); } } else { if (this.isSysDarkPrefer) { this.setLight(); } } } let self = this; /* always follow the system prefers */ this.sysDarkPrefers.addEventListener("change", () => { if (self.hasMode) { if (self.isDarkMode) { if (!self.isSysDarkPrefer) { self.setDark(); } } else { if (self.isSysDarkPrefer) { self.setLight(); } } self.clearMode(); } self.notify(); }); } /* constructor() */ get sysDarkPrefers() { return window.matchMedia("(prefers-color-scheme: dark)"); } get isSysDarkPrefer() { return this.sysDarkPrefers.matches; } get isDarkMode() { return this.mode === ModeToggle.DARK_MODE; } get isLightMode() { return this.mode === ModeToggle.LIGHT_MODE; } get hasMode() { return this.mode != null; } get mode() { return sessionStorage.getItem(ModeToggle.MODE_KEY); } /* get the current mode on screen */ get modeStatus() { if (this.isDarkMode || (!this.hasMode && this.isSysDarkPrefer)) { return ModeToggle.DARK_MODE; } else { return ModeToggle.LIGHT_MODE; } } setDark() { $('html').attr(ModeToggle.MODE_ATTR, ModeToggle.DARK_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); } setLight() { $('html').attr(ModeToggle.MODE_ATTR, ModeToggle.LIGHT_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); } clearMode() { $('html').removeAttr(ModeToggle.MODE_ATTR); sessionStorage.removeItem(ModeToggle.MODE_KEY); } /* Notify another plugins that the theme mode has changed */ notify() { window.postMessage({ direction: ModeToggle.ID, message: this.modeStatus }, "*"); } } /* ModeToggle */ const toggle = new ModeToggle(); function flipMode() { if (toggle.hasMode) { if (toggle.isSysDarkPrefer) { if (toggle.isLightMode) { toggle.clearMode(); } else { toggle.setLight(); } } else { if (toggle.isDarkMode) { toggle.clearMode(); } else { toggle.setDark(); } } } else { if (toggle.isSysDarkPrefer) { toggle.setLight(); } else { toggle.setDark(); } } toggle.notify(); } /* flipMode() */ </script><body data-spy="scroll" data-target="#toc" data-topbar-visible="true"><div id="sidebar" class="d-flex flex-column align-items-end"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" class="mx-auto"> <img src="https://avatars.githubusercontent.com/u/76043326?s=400&u=1f9b3c508db6b5caec736e7c1edb42ba76e2f2a1&v=4" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/">Yunpeng Tai</a></div><div class="site-subtitle font-italic">Learn to think</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tag ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fas fa-info-circle ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center align-items-center"> <button class="mode-toggle btn" aria-label="Switch Mode"> <i class="fas fa-adjust"></i> </button> <span class="icon-border"></span> <a href="https://github.com/sherlcok314159" aria-label="github" target="_blank" rel="noopener"> <i class="fab fa-github"></i> </a> <a href="https://twitter.com/TonySta14611077" aria-label="twitter" target="_blank" rel="noopener"> <i class="fab fa-twitter"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['yunpengtai.typ','gmail.com'].join('@')" aria-label="email" > <i class="fas fa-envelope"></i> </a></div></div><div id="topbar-wrapper"><div id="topbar" class="container d-flex align-items-center justify-content-between h-100 pl-3 pr-3 pl-md-4 pr-md-4"> <span id="breadcrumb"> <span> <a href="/"> Home </a> </span> <span>Diving in distributed training in PyTorch</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..."> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper" class="d-flex justify-content-center"><div id="main" class="container pl-xl-4 pr-xl-4"><div class="row"><div id="core-wrapper" class="col-12 col-lg-11 col-xl-9 pr-xl-4"><div class="post pl-1 pr-1 pl-md-2 pr-md-2"><h1 data-toc-skip>Diving in distributed training in PyTorch</h1><div class="post-meta text-muted"> <span> Posted <em class="" data-ts="1668951420" data-df="ll" data-toggle="tooltip" data-placement="bottom"> Nov 20, 2022 </em> </span> <span> Updated <em class="" data-ts="1669080195" data-df="ll" data-toggle="tooltip" data-placement="bottom"> Nov 22, 2022 </em> </span><div class="mt-3 mb-3"> <img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 600 100'%3E%3C/svg%3E" data-src="https://s2.loli.net//2022/11/20/xKhQypW5SeY4oEt.jpg" class="preview-img bg" alt="Preview Image" width="600" height="100" data-proofer-ignore></div><div class="d-flex justify-content-between"> <span></span><div> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="4632 words"> <em>25 min</em> read</span></div></div></div><div class="post-content"><p>鉴于网上此类教程有不少模糊不清，对原理不得其法，代码也难跑通，故而花了几天细究了一下相关原理和实现，欢迎批评指正！</p><blockquote class="prompt-tip"><div><p>关于此部分的代码，可以去<a href="https://github.com/sherlcok314159/dl-tools">这里</a>查看</p></div></blockquote><p><strong>在开始前，我需要特别致谢一下一位挚友，他送了我双显卡的机器来赞助我做个人研究，否则多卡的相关实验就得付费在云平台上跑了，感谢好朋友一路以来的支持，这份恩情值得一辈子铭记！这篇文章作为礼物赠与挚友。</strong></p><h1 id="why-parallel">Why Parallel</h1><p>我们在两种情况下进行并行化训练<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup>：</p><ol><li><strong>模型一张卡放不下</strong>：我们需要将模型不同的结构放置到不同的GPU上运行，这种情况叫<code class="language-plaintext highlighter-rouge">ModelParallel(MP)</code><li><strong>一张卡的batch size(bs)过小</strong>：有些时候数据的最大长度调的比较高（e.g., 512），可用的bs就很小，较小的bs会导致收敛不稳定，因而将数据分发到多个GPU上进行并行训练，这种情况叫<code class="language-plaintext highlighter-rouge">DataParallel(DP)</code>。当然，DP肯定还可以加速训练，常见于大模型的训练中</ol><p>这里只讲一下DP在pytorch中的原理和相关实现，即DataParallel和DistributedParallel</p><h1 id="data-parallel">Data Parallel</h1><h2 id="实现原理"><span class="mr-2">实现原理</span><a href="#实现原理" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>实现就是循环往复一个过程：数据分发，模型复制，各自前向传播，汇聚输出，计算损失，梯度回传，梯度汇聚更新，可以参见下图<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup>：</p><p><img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 400 700'%3E%3C/svg%3E" data-src="https://s2.loli.net/2022/11/20/cGkHbLx8S7jlpOd.png" alt="dp.png" width="400" height="700" data-proofer-ignore></p><p>pytorch中部分关键源码<sup id="fnref:3" role="doc-noteref"><a href="#fn:3" class="footnote" rel="footnote">3</a></sup>截取如下：</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
</pre><td class="rouge-code"><pre><span class="k">def</span> <span class="nf">data_parallel</span><span class="p">(</span>
	<span class="n">module</span><span class="p">,</span> 
	<span class="nb">input</span><span class="p">,</span> 
	<span class="n">device_ids</span><span class="p">,</span> 
	<span class="n">output_device</span><span class="o">=</span><span class="bp">None</span>
<span class="p">):</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">device_ids</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">module</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">output_device</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">output_device</span> <span class="o">=</span> <span class="n">device_ids</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

    <span class="c1"># 复制模型
</span>    <span class="n">replicas</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">parallel</span><span class="p">.</span><span class="n">replicate</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">device_ids</span><span class="p">)</span>
    <span class="c1"># 拆分数据
</span>    <span class="n">inputs</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">parallel</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">device_ids</span><span class="p">)</span>
    <span class="n">replicas</span> <span class="o">=</span> <span class="n">replicas</span><span class="p">[:</span><span class="nb">len</span><span class="p">(</span><span class="n">inputs</span><span class="p">)]</span>
    <span class="c1"># 各自前向传播
</span>    <span class="n">outputs</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">parallel</span><span class="p">.</span><span class="n">parallel_apply</span><span class="p">(</span><span class="n">replicas</span><span class="p">,</span> <span class="n">inputs</span><span class="p">)</span>
    <span class="c1"># 汇聚输出
</span>    <span class="k">return</span> <span class="n">nn</span><span class="p">.</span><span class="n">parallel</span><span class="p">.</span><span class="n">gather</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">output_device</span><span class="p">)</span>
</pre></table></code></div></div><h2 id="代码使用"><span class="mr-2">代码使用</span><a href="#代码使用" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><blockquote class="prompt-warning"><div><p>因为运行时会将数据平均拆分到GPU上，所以我们准备数据的时候， batch size = per_gpu_batch_size * n_gpus</p></div></blockquote><p>同时，需要注意主GPU需要进行汇聚等操作，因而需要比单卡运行时<code class="language-plaintext highlighter-rouge">多留出一些空间</code></p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
</pre><td class="rouge-code"><pre><span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="c1"># device_ids默认所有可使用的设备
# output_device默认cuda:0
</span><span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">DataParallel</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">device_ids</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> 
                      <span class="n">output_device</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="c1"># input_var can be on any device, including CPU
</span><span class="n">output</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">input_var</span><span class="p">)</span>
</pre></table></code></div></div><p>接下来看个更详细的例子<sup id="fnref:4" role="doc-noteref"><a href="#fn:4" class="footnote" rel="footnote">4</a></sup>，需要注意的是被DP包裹之后涉及到模型相关的，需要调用DP.module，比如<code class="language-plaintext highlighter-rouge">加载模型</code></p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
</pre><td class="rouge-code"><pre><span class="k">class</span> <span class="nc">Model</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="c1"># Our model
</span>    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Model</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="c1"># for convenience
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">fc</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">fc</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="s">"</span><span class="se">\t</span><span class="s">In Model: input size"</span><span class="p">,</span> <span class="nb">input</span><span class="p">.</span><span class="n">size</span><span class="p">(),</span>
              <span class="s">"output size"</span><span class="p">,</span> <span class="n">output</span><span class="p">.</span><span class="n">size</span><span class="p">())</span>
        <span class="k">return</span> <span class="n">output</span>

<span class="n">bs</span><span class="p">,</span> <span class="n">input_size</span><span class="p">,</span> <span class="n">output_size</span> <span class="o">=</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">10</span>
<span class="c1"># define inputs
</span><span class="n">inputs</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">((</span><span class="n">bs</span><span class="p">,</span> <span class="n">input_size</span><span class="p">)).</span><span class="n">cuda</span><span class="p">()</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">)</span>
<span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">device_count</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
  <span class="k">print</span><span class="p">(</span><span class="s">"Let's use"</span><span class="p">,</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">device_count</span><span class="p">(),</span> <span class="s">"GPUs!"</span><span class="p">)</span>
  <span class="c1"># dim = 0 [6, xxx] -&gt; [2, ...], [2, ...], [2, ...] on 3 GPUs
</span>  <span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">DataParallel</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
<span class="c1"># 先DataParallel，再cuda
</span><span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">cuda</span><span class="p">()</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Outside: input size"</span><span class="p">,</span> <span class="n">inputs</span><span class="p">.</span><span class="n">size</span><span class="p">(),</span>
	  <span class="s">"output_size"</span><span class="p">,</span> <span class="n">outputs</span><span class="p">.</span><span class="n">size</span><span class="p">())</span>
<span class="c1"># assume 2 GPUS are available
# Let's use 2 GPUs!
#    In Model: input size torch.Size([3, 8]) output size torch.Size([3, 10])
#    In Model: input size torch.Size([3, 8]) output size torch.Size([3, 10])
# Outside: input size torch.Size([6, 8]) output_size torch.Size([6, 10])
</span>
<span class="c1"># save the model
</span><span class="n">torch</span><span class="p">.</span><span class="n">save</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="n">module</span><span class="p">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="n">PATH</span><span class="p">)</span>
<span class="c1"># load again
</span><span class="n">model</span><span class="p">.</span><span class="n">module</span><span class="p">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="n">PATH</span><span class="p">))</span>
<span class="c1"># do anything you want
</span></pre></table></code></div></div><p>如果经常使用huggingface，这里有两个误区需要小心：</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
</pre><td class="rouge-code"><pre><span class="c1"># data parallel object has no save_pretrained
</span><span class="n">model</span> <span class="o">=</span> <span class="n">xxx</span><span class="p">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">PATH</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">DataParallel</span><span class="p">(</span><span class="n">model</span><span class="p">).</span><span class="n">cuda</span><span class="p">()</span>
<span class="n">model</span><span class="p">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="n">NEW_PATH</span><span class="p">)</span> <span class="c1"># error
# 因为model被DP wrap了，得先取出模型 #
</span><span class="n">model</span><span class="p">.</span><span class="n">module</span><span class="p">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="n">NEW_PATH</span><span class="p">)</span>
</pre></table></code></div></div><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
</pre><td class="rouge-code"><pre><span class="c1"># HF实现貌似是返回N个loss（N为GPU数量）
# 然后对N个loss取mean
</span><span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">)</span>
<span class="n">loss</span><span class="p">,</span> <span class="n">logits</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">.</span><span class="n">loss</span><span class="p">,</span> <span class="n">outputs</span><span class="p">.</span><span class="n">logits</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span><span class="p">.</span><span class="n">mean</span><span class="p">()</span>
<span class="n">loss</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>

<span class="c1"># 返回的logits是汇聚后的
# HF实现和我们手动算loss有细微差异
# 手动算略好于HF
</span><span class="n">loss2</span> <span class="o">=</span> <span class="n">loss_fct</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">loss</span> <span class="o">!=</span> <span class="n">loss2</span>
<span class="bp">True</span>
</pre></table></code></div></div><h2 id="显存不均匀"><span class="mr-2">显存不均匀</span><a href="#显存不均匀" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>了解前面的原理后，就会明白为什么会显存不均匀。因为<code class="language-plaintext highlighter-rouge">GPU0</code>比其他GPU多了汇聚的工作，得留一些显存，而其他GPU显然是不需要的。那么，解决方案就是让其他GPU的batch size开大点，<code class="language-plaintext highlighter-rouge">GPU0</code>维持原状，即不按照默认实现的<code class="language-plaintext highlighter-rouge">平分数据</code></p><p>首先我们继承原来的DataParallel（此处参考<sup id="fnref:5" role="doc-noteref"><a href="#fn:5" class="footnote" rel="footnote">5</a></sup>），这里我们给定第一个GPU的bs就可以，这个是实际的bs而不是乘上梯度后的。假如你想要总的bs为64，梯度累积为2，一共2张GPU，而一张最多只能18，那么保险一点<code class="language-plaintext highlighter-rouge">GPU0</code>设置为14，GPU1是18，也就是说你DataLoader每个batch大小是32，<code class="language-plaintext highlighter-rouge">gpu0_bsz=14</code></p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
</pre><td class="rouge-code"><pre><span class="k">class</span> <span class="nc">BalancedDataParallel</span><span class="p">(</span><span class="n">DataParallel</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">gpu0_bsz</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">gpu0_bsz</span> <span class="o">=</span> <span class="n">gpu0_bsz</span>
        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</pre></table></code></div></div><p>核心代码就在于我们重新分配chunk_sizes，实现思路就是将总的减去第一个GPU的再除以剩下的设备，源码的话有些死板，用的时候不妨参考我的<sup id="fnref:6" role="doc-noteref"><a href="#fn:6" class="footnote" rel="footnote">6</a></sup></p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
</pre><td class="rouge-code"><pre><span class="k">def</span> <span class="nf">scatter</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="n">device_ids</span><span class="p">):</span>
    <span class="c1"># 不同于源码，获取batch size更加灵活
</span>    <span class="c1"># 支持只有kwargs的情况，如model(**inputs)
</span>    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">bsz</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">size</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">dim</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">kwargs</span><span class="p">:</span>
        <span class="n">bsz</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">kwargs</span><span class="p">.</span><span class="n">values</span><span class="p">())[</span><span class="mi">0</span><span class="p">].</span><span class="n">size</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">dim</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="nb">ValueError</span><span class="p">(</span><span class="s">"You must pass inputs to the model!"</span><span class="p">)</span>

    <span class="n">num_dev</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">device_ids</span><span class="p">)</span>
    <span class="n">gpu0_bsz</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">gpu0_bsz</span>
    <span class="c1"># 除第一块之外每块GPU的bsz
</span>    <span class="n">bsz_unit</span> <span class="o">=</span> <span class="p">(</span><span class="n">bsz</span> <span class="o">-</span> <span class="n">gpu0_bsz</span><span class="p">)</span> <span class="o">//</span> <span class="p">(</span><span class="n">num_dev</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">gpu0_bsz</span> <span class="o">&lt;</span> <span class="n">bsz_unit</span><span class="p">:</span>
        <span class="c1"># adapt the chunk sizes
</span>        <span class="n">chunk_sizes</span> <span class="o">=</span> <span class="p">[</span><span class="n">gpu0_bsz</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="n">bsz_unit</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="n">num_dev</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">delta</span> <span class="o">=</span> <span class="n">bsz</span> <span class="o">-</span> <span class="nb">sum</span><span class="p">(</span><span class="n">chunk_sizes</span><span class="p">)</span>
        <span class="c1"># 补足偏移量
</span>        <span class="c1"># 会有显存溢出的风险，因而最好给定的bsz是可以整除的
</span>        <span class="c1"># e.g., 总的=52 =&gt; bsz_0=16, bsz_1=bsz_2=18
</span>        <span class="c1"># 总的=53 =&gt; bsz_0=16, bsz_1=19, bsz_2=18
</span>        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">delta</span><span class="p">):</span>
            <span class="n">chunk_sizes</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">if</span> <span class="n">gpu0_bsz</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">chunk_sizes</span> <span class="o">=</span> <span class="n">chunk_sizes</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">().</span><span class="n">scatter</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="n">device_ids</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">scatter_kwargs</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="n">device_ids</span><span class="p">,</span> <span class="n">chunk_sizes</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">dim</span><span class="p">)</span>
</pre></table></code></div></div><h2 id="优缺点"><span class="mr-2">优缺点</span><a href="#优缺点" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><ul><li>优点：便于操作，理解简单<li>缺点：GPU分配不均匀；每次更新完都得销毁<strong>线程</strong>（运行程序后会有一个进程，一个进程可以有很多个线程）重新复制模型，因而速度慢</ul><h1 id="distributed-data-parallel">Distributed Data Parallel</h1><h2 id="实现原理-1"><span class="mr-2">实现原理</span><a href="#实现原理-1" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><ol><li>与DataParallel不同的是，Distributed Data Parallel会开设多个进程而非线程，进程数 = GPU数，每个进程都可以独立进行训练，也就是说代码的所有部分都会被每个进程同步调用，如果你某个地方print张量，你会发现device的差异<li>sampler会将数据按照进程数切分，<strong>确保不同进程的数据不同</strong><li>每个进程独立进行前向训练<li>每个进程利用Ring All-Reduce进行通信，将梯度信息进行聚合<li>每个进程同步更新模型参数，进行新一轮训练</ol><h3 id="按进程切分"><span class="mr-2">按进程切分</span><a href="#按进程切分" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>如何确保数据不同呢？不妨看看DistributedSampler的源码</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
</pre><td class="rouge-code"><pre><span class="c1"># 判断数据集长度是否可以整除GPU数
# 如果不能，选择舍弃还是补全，进而决定总数
# If the dataset length is evenly divisible by # of replicas
# then there is no need to drop any data, since the dataset 
# will be split equally.
</span><span class="k">if</span> <span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">drop_last</span> <span class="ow">and</span> 
	<span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">dataset</span><span class="p">)</span> <span class="o">%</span> <span class="bp">self</span><span class="p">.</span><span class="n">num_replicas</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">):</span>
	<span class="c1"># num_replicas = num_gpus
</span>    <span class="bp">self</span><span class="p">.</span><span class="n">num_samples</span> <span class="o">=</span> <span class="n">math</span><span class="p">.</span><span class="n">ceil</span><span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">dataset</span><span class="p">)</span> <span class="o">-</span> 
        <span class="bp">self</span><span class="p">.</span><span class="n">num_replicas</span><span class="p">)</span> <span class="o">/</span><span class="bp">self</span><span class="p">.</span><span class="n">num_replicas</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">num_samples</span> <span class="o">=</span> <span class="n">math</span><span class="p">.</span><span class="n">ceil</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">dataset</span><span class="p">)</span> <span class="o">/</span> 
        <span class="bp">self</span><span class="p">.</span><span class="n">num_replicas</span><span class="p">)</span> 
<span class="bp">self</span><span class="p">.</span><span class="n">total_size</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">num_samples</span> <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">num_replicas</span>

<span class="c1"># 根据是否shuffle来创建indices
</span><span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">shuffle</span><span class="p">:</span>
    <span class="c1"># deterministically shuffle based on epoch and seed
</span>    <span class="n">g</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">Generator</span><span class="p">()</span>
    <span class="n">g</span><span class="p">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">seed</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">epoch</span><span class="p">)</span>
    <span class="n">indices</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">randperm</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">dataset</span><span class="p">),</span> <span class="n">generator</span><span class="o">=</span><span class="n">g</span><span class="p">).</span><span class="n">tolist</span><span class="p">()</span>  
<span class="k">else</span><span class="p">:</span>
    <span class="n">indices</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">dataset</span><span class="p">)))</span>  
<span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="p">.</span><span class="n">drop_last</span><span class="p">:</span>
    <span class="c1"># add extra samples to make it evenly divisible
</span>    <span class="n">padding_size</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">total_size</span> <span class="o">-</span> <span class="nb">len</span><span class="p">(</span><span class="n">indices</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">padding_size</span> <span class="o">&lt;=</span> <span class="nb">len</span><span class="p">(</span><span class="n">indices</span><span class="p">):</span>
        <span class="c1"># 不够就按indices顺序加
</span>        <span class="c1"># e.g., indices为[0, 1, 2, 3 ...]，而padding_size为4
</span>        <span class="c1"># 加好之后的indices[..., 0, 1, 2, 3]
</span>        <span class="n">indices</span> <span class="o">+=</span> <span class="n">indices</span><span class="p">[:</span><span class="n">padding_size</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">indices</span> <span class="o">+=</span> <span class="p">(</span><span class="n">indices</span> <span class="o">*</span> <span class="n">math</span><span class="p">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">padding_size</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">indices</span><span class="p">)))[:</span><span class="n">padding_size</span><span class="p">]</span>
<span class="k">else</span><span class="p">:</span>
    <span class="c1"># remove tail of data to make it evenly divisible.
</span>    <span class="n">indices</span> <span class="o">=</span> <span class="n">indices</span><span class="p">[:</span><span class="bp">self</span><span class="p">.</span><span class="n">total_size</span><span class="p">]</span>
<span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">indices</span><span class="p">)</span> <span class="o">==</span> <span class="bp">self</span><span class="p">.</span><span class="n">total_size</span>
<span class="c1"># subsample
# rank代表进程id
</span><span class="n">indices</span> <span class="o">=</span> <span class="n">indices</span><span class="p">[</span><span class="bp">self</span><span class="p">.</span><span class="n">rank</span><span class="p">:</span><span class="bp">self</span><span class="p">.</span><span class="n">total_size</span><span class="p">:</span><span class="bp">self</span><span class="p">.</span><span class="n">num_replicas</span><span class="p">]</span>
<span class="k">return</span> <span class="nb">iter</span><span class="p">(</span><span class="n">indices</span><span class="p">)</span>
</pre></table></code></div></div><h3 id="ring-all-reduce"><span class="mr-2">Ring All-Reduce</span><a href="#ring-all-reduce" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>那么什么是<strong>Ring All-Reduce</strong>呢？又为啥可以降低通信成本呢？</p><p>首先将每块GPU上的梯度拆分成四个部分，比如$g_0 = [a_0; b_0; c_0; d_0]$，如下图（此部分原理致谢下王老师，讲的很清晰<sup id="fnref:7" role="doc-noteref"><a href="#fn:7" class="footnote" rel="footnote">7</a></sup>）：</p><p><img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 500 750'%3E%3C/svg%3E" data-src="https://s2.loli.net/2022/11/20/q72OKSHhmuXYWvN.png" alt="torch_ddp1.png" width="500" height="750" data-proofer-ignore></p><p>所有GPU的传播都是<strong>同步</strong>进行的，传播的规律有两条：</p><ol><li>只与自己<code class="language-plaintext highlighter-rouge">下一个位置</code>的GPU进行通信，比如<code class="language-plaintext highlighter-rouge">0 &gt; 1，3 &gt; 0</code><li>四个部分，哪块GPU上占的多，就由该块GPU往它下一个传，初始从主节点传播，即<code class="language-plaintext highlighter-rouge">GPU0</code>，你可以想象跟接力一样，a传b，b负责传给c</ol><p>第一次传播如下：</p><p><img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 500 750'%3E%3C/svg%3E" data-src="https://s2.loli.net/2022/11/20/MNWF2dtB7wOsIoK.png" alt="torch_ddp2.png" width="500" height="750" data-proofer-ignore></p><p>那么结果就是：</p><p><img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 500 750'%3E%3C/svg%3E" data-src="https://s2.loli.net/2022/11/20/qj1VHUlSDiXstTz.png" alt="torch_ddp3.png" width="500" height="750" data-proofer-ignore></p><p>那么，按照谁多谁往下传的原则，此时应该是GPU1往GPU2传a0和a1，GPU2往GPU3传b1和b2，以此类推</p><p><img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 500 750'%3E%3C/svg%3E" data-src="https://s2.loli.net/2022/11/20/4mfqWSMjO3IokxH.png" alt="torch_ddp4.png" width="500" height="750" data-proofer-ignore></p><p>接下来再传播就会有GPU3 a的部分全有，<code class="language-plaintext highlighter-rouge">GPU0</code>上b的部分全有等，就再往下传</p><p><img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 500 750'%3E%3C/svg%3E" data-src="https://s2.loli.net/2022/11/20/v3jzp4PIQSYERZy.png" alt="torch_ddp5.png" width="500" height="750" data-proofer-ignore></p><p>再来几遍便可以使得每块GPU上都获得了来自其他GPU的梯度啦</p><p><img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 500 750'%3E%3C/svg%3E" data-src="https://s2.loli.net/2022/11/20/OA2Ikvxt59YGiVH.png" alt="torch_ddp6.png" width="500" height="750" data-proofer-ignore></p><h2 id="代码使用-1"><span class="mr-2">代码使用</span><a href="#代码使用-1" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><h3 id="基础概念"><span class="mr-2">基础概念</span><a href="#基础概念" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>第一个是后端的选择，即数据传输协议，从下表可以看出<sup id="fnref:8" role="doc-noteref"><a href="#fn:8" class="footnote" rel="footnote">8</a></sup>，当使用CPU时可以选择<code class="language-plaintext highlighter-rouge">gloo</code>而GPU则可以是<code class="language-plaintext highlighter-rouge">nccl</code></p><div class="table-wrapper"><table><thead><tr><th style="text-align: center"><strong>Backend</strong><th style="text-align: center"><strong>gloo</strong><th style="text-align: center"> <th style="text-align: center"><strong>mpi</strong><th style="text-align: center"> <th style="text-align: center"><strong>nccl</strong><th style="text-align: center"> <tbody><tr><td style="text-align: center">Device<td style="text-align: center">CPU<td style="text-align: center">GPU<td style="text-align: center">CPU<td style="text-align: center">GPU<td style="text-align: center">CPU<td style="text-align: center">GPU<tr><td style="text-align: center">send<td style="text-align: center">✓<td style="text-align: center">✘<td style="text-align: center">✓<td style="text-align: center">?<td style="text-align: center">✘<td style="text-align: center">✓<tr><td style="text-align: center">recv<td style="text-align: center">✓<td style="text-align: center">✘<td style="text-align: center">✓<td style="text-align: center">?<td style="text-align: center">✘<td style="text-align: center">✓<tr><td style="text-align: center">broadcast<td style="text-align: center">✓<td style="text-align: center">✓<td style="text-align: center">✓<td style="text-align: center">?<td style="text-align: center">✘<td style="text-align: center">✓<tr><td style="text-align: center">all_reduce<td style="text-align: center">✓<td style="text-align: center">✓<td style="text-align: center">✓<td style="text-align: center">?<td style="text-align: center">✘<td style="text-align: center">✓<tr><td style="text-align: center">reduce<td style="text-align: center">✓<td style="text-align: center">✘<td style="text-align: center">✓<td style="text-align: center">?<td style="text-align: center">✘<td style="text-align: center">✓<tr><td style="text-align: center">all_gather<td style="text-align: center">✓<td style="text-align: center">✘<td style="text-align: center">✓<td style="text-align: center">?<td style="text-align: center">✘<td style="text-align: center">✓<tr><td style="text-align: center">gather<td style="text-align: center">✓<td style="text-align: center">✘<td style="text-align: center">✓<td style="text-align: center">?<td style="text-align: center">✘<td style="text-align: center">✓<tr><td style="text-align: center">scatter<td style="text-align: center">✓<td style="text-align: center">✘<td style="text-align: center">✓<td style="text-align: center">?<td style="text-align: center">✘<td style="text-align: center">✘<tr><td style="text-align: center">reduce_scatter<td style="text-align: center">✘<td style="text-align: center">✘<td style="text-align: center">✘<td style="text-align: center">✘<td style="text-align: center">✘<td style="text-align: center">✓<tr><td style="text-align: center">all_to_all<td style="text-align: center">✘<td style="text-align: center">✘<td style="text-align: center">✓<td style="text-align: center">?<td style="text-align: center">✘<td style="text-align: center">✓<tr><td style="text-align: center">barrier<td style="text-align: center">✓<td style="text-align: center">✘<td style="text-align: center">✓<td style="text-align: center">?<td style="text-align: center">✘<td style="text-align: center">✓</table></div><p>接下来是一些参数的解释<sup id="fnref:9" role="doc-noteref"><a href="#fn:9" class="footnote" rel="footnote">9</a></sup>：</p><div class="table-wrapper"><table><thead><tr><th style="text-align: center">Arg<th style="text-align: center">Meaning<tbody><tr><td style="text-align: center">group<td style="text-align: center">一次发起的所有进程构成一个group，除非想更精细通信，创建new_group<tr><td style="text-align: center">world_size<td style="text-align: center">一个group中进程数目，即为GPU的数量<tr><td style="text-align: center">rank<td style="text-align: center">进程id，主节点<code class="language-plaintext highlighter-rouge">rank=0</code>，其他的在<code class="language-plaintext highlighter-rouge">0</code>和world_size-1之间<tr><td style="text-align: center">local_rank<td style="text-align: center">进程在本地节点/机器的id</table></div><p>举个例子，假如你有两台服务器（又被称为node），每台服务器有4张GPU，那么，world_size即为8，<code class="language-plaintext highlighter-rouge">rank=[0, 1, 2, 3, 4, 5, 6, 7]</code>, 每个服务器上的进程的local_rank为<code class="language-plaintext highlighter-rouge">[0, 1, 2, 3]</code></p><p>然后是<strong>初始化方法</strong>的选择，有<code class="language-plaintext highlighter-rouge">TCP</code>和<code class="language-plaintext highlighter-rouge">共享文件</code>两种，一般指定<code class="language-plaintext highlighter-rouge">rank=0</code>为master节点</p><p>TCP显而易见是通过网络进行传输，需要指定主节点的ip（可以为主节点实际IP，或者是localhost）和空闲的端口</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
</pre><td class="rouge-code"><pre><span class="kn">import</span> <span class="nn">torch.distributed</span> <span class="k">as</span> <span class="n">dist</span>

<span class="n">dist</span><span class="p">.</span><span class="n">init_process_group</span><span class="p">(</span><span class="n">backend</span><span class="p">,</span> <span class="n">init_method</span><span class="o">=</span><span class="s">'tcp://ip:port'</span><span class="p">,</span>
                        <span class="n">rank</span><span class="o">=</span><span class="n">rank</span><span class="p">,</span> <span class="n">world_size</span><span class="o">=</span><span class="n">world_size</span><span class="p">)</span>
</pre></table></code></div></div><p>共享文件的话需要手动删除上次启动时残留的文件，加上官方有一堆警告，还是建议使用TCP</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre><td class="rouge-code"><pre><span class="n">dist</span><span class="p">.</span><span class="n">init_process_group</span><span class="p">(</span><span class="n">backend</span><span class="p">,</span> <span class="n">init_method</span><span class="o">=</span><span class="s">'file://Path'</span><span class="p">,</span> 
                        <span class="n">rank</span><span class="o">=</span><span class="n">rank</span><span class="p">,</span> <span class="n">world_size</span><span class="o">=</span><span class="n">world_size</span><span class="p">)</span>
</pre></table></code></div></div><h3 id="launch方法"><span class="mr-2">launch方法</span><a href="#launch方法" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><h4 id="初始化"><span class="mr-2">初始化</span><a href="#初始化" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4><p>这里先讲用launch的方法，关于torch.multiprocessing留到后面讲</p><p>在启动后，rank和world_size都会自动被DDP写入环境中，可以提前准备好参数类，如<code class="language-plaintext highlighter-rouge">argparse</code>这种</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
</pre><td class="rouge-code"><pre><span class="n">args</span><span class="p">.</span><span class="n">rank</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">[</span><span class="s">'RANK'</span><span class="p">])</span>
<span class="n">args</span><span class="p">.</span><span class="n">world_size</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">[</span><span class="s">'WORLD_SIZE'</span><span class="p">])</span>
<span class="n">args</span><span class="p">.</span><span class="n">local_rank</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">[</span><span class="s">'LOCAL_RANK'</span><span class="p">])</span>
</pre></table></code></div></div><p>首先，在使用<code class="language-plaintext highlighter-rouge">distributed</code>包的任何其他函数之前，按照tcp方法进行初始化，需要注意的是需要手动指定一共可用的设备<code class="language-plaintext highlighter-rouge">CUDA_VISIBLE_DEVICES</code></p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
</pre><td class="rouge-code"><pre><span class="k">def</span> <span class="nf">dist_setup_launch</span><span class="p">(</span><span class="n">args</span><span class="p">):</span>
    <span class="c1"># tell DDP available devices [NECESSARY]
</span>    <span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">[</span><span class="s">'CUDA_VISIBLE_DEVICES'</span><span class="p">]</span> <span class="o">=</span> <span class="n">args</span><span class="p">.</span><span class="n">devices</span>
    <span class="n">args</span><span class="p">.</span><span class="n">rank</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">[</span><span class="s">'RANK'</span><span class="p">])</span>
    <span class="n">args</span><span class="p">.</span><span class="n">world_size</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">[</span><span class="s">'WORLD_SIZE'</span><span class="p">])</span>
    <span class="n">args</span><span class="p">.</span><span class="n">local_rank</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">[</span><span class="s">'LOCAL_RANK'</span><span class="p">])</span>

    <span class="n">dist</span><span class="p">.</span><span class="n">init_process_group</span><span class="p">(</span><span class="n">args</span><span class="p">.</span><span class="n">backend</span><span class="p">,</span> 
                            <span class="n">args</span><span class="p">.</span><span class="n">init_method</span><span class="p">,</span>
                            <span class="n">rank</span><span class="o">=</span><span class="n">args</span><span class="p">.</span><span class="n">rank</span><span class="p">,</span>
                            <span class="n">world_size</span><span class="o">=</span><span class="n">args</span><span class="p">.</span><span class="n">world_size</span><span class="p">)</span>
    <span class="c1"># this is optional, otherwise you may need to specify the 
</span>    <span class="c1"># device when you move something e.g., model.cuda(1) 
</span>    <span class="c1"># or model.to(args.rank)
</span>    <span class="c1"># Setting device makes things easy: model.cuda()
</span>    <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">set_device</span><span class="p">(</span><span class="n">args</span><span class="p">.</span><span class="n">rank</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s">'The Current Rank is %d | The Total Ranks are %d'</span> 
          <span class="o">%</span><span class="p">(</span><span class="n">args</span><span class="p">.</span><span class="n">rank</span><span class="p">,</span> <span class="n">args</span><span class="p">.</span><span class="n">world_size</span><span class="p">))</span>
</pre></table></code></div></div><h4 id="distributedsampler"><span class="mr-2">DistributedSampler</span><a href="#distributedsampler" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4><p>接下来创建DistributedSampler，是否pin_memory，根据你本机的内存决定。pin_memory的意思是提前在内存中申请一部分专门存放Tensor。假如说你内存比较小，就会跟虚拟内存，即硬盘进行交换，这样转义到GPU上会比内存直接到GPU耗时。</p><p>因而，如果你的内存比较大，可以设置为True；然而，如果开了导致卡顿的情况，建议关闭</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
</pre><td class="rouge-code"><pre><span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span><span class="p">,</span> <span class="n">DistributedSampler</span>

<span class="n">train_sampler</span> <span class="o">=</span> <span class="n">DistributedSampler</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="n">args</span><span class="p">.</span><span class="n">seed</span><span class="p">)</span>
<span class="n">train_dataloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">,</span>
                              <span class="n">pin_memory</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
                              <span class="n">shuffle</span><span class="o">=</span><span class="p">(</span><span class="n">train_sampler</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">),</span>
                              <span class="n">batch_size</span><span class="o">=</span><span class="n">args</span><span class="p">.</span><span class="n">per_gpu_train_bs</span><span class="p">,</span>
                              <span class="n">num_workers</span><span class="o">=</span><span class="n">args</span><span class="p">.</span><span class="n">num_workers</span><span class="p">,</span>
                              <span class="n">sampler</span><span class="o">=</span><span class="n">train_sampler</span><span class="p">)</span>

<span class="n">eval_sampler</span> <span class="o">=</span> <span class="n">DistributedSampler</span><span class="p">(</span><span class="n">eval_dataset</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="n">args</span><span class="p">.</span><span class="n">seed</span><span class="p">)</span>
<span class="n">eval_dataloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">eval_dataset</span><span class="p">,</span>
                             <span class="n">pin_memory</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
                             <span class="n">batch_size</span><span class="o">=</span><span class="n">args</span><span class="p">.</span><span class="n">per_gpu_eval_bs</span><span class="p">,</span>
                             <span class="n">num_workers</span><span class="o">=</span><span class="n">args</span><span class="p">.</span><span class="n">num_workers</span><span class="p">,</span>
                             <span class="n">sampler</span><span class="o">=</span><span class="n">eval_sampler</span><span class="p">)</span>
</pre></table></code></div></div><h4 id="加载模型"><span class="mr-2">加载模型</span><a href="#加载模型" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4><p>然后加载模型，跟DataParallel不同的是需要提前放置到cuda上，还记得上面关于设置cuda_device的语句嘛，因为设置好之后每个进程只能看见一个GPU，所以直接<code class="language-plaintext highlighter-rouge">model.cuda()</code>，不需要指定device</p><p>同时，我们必须给DDP提示目前是哪个rank</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
</pre><td class="rouge-code"><pre><span class="kn">from</span> <span class="nn">torch.nn.parallel</span> <span class="kn">import</span> <span class="n">DistributedDataParallel</span> <span class="k">as</span> <span class="n">DDP</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">cuda</span><span class="p">()</span>
<span class="c1"># tell DDP which rank
</span><span class="n">model</span> <span class="o">=</span> <span class="n">DDP</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">find_unused_parameters</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">device_ids</span><span class="o">=</span><span class="p">[</span><span class="n">rank</span><span class="p">])</span>
</pre></table></code></div></div><p>注意，当模型带有Batch Norm时：</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre><td class="rouge-code"><pre><span class="k">if</span> <span class="n">args</span><span class="p">.</span><span class="n">syncBN</span><span class="p">:</span>
    <span class="n">nn</span><span class="p">.</span><span class="n">SyncBatchNorm</span><span class="p">.</span><span class="n">convert_sync_batchnorm</span><span class="p">(</span><span class="n">model</span><span class="p">).</span><span class="n">cuda</span><span class="p">()</span>
</pre></table></code></div></div><h4 id="训练相关"><span class="mr-2">训练相关</span><a href="#训练相关" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4><p>每个epoch开始训练的时候，记得用sampler的set_epoch，这样使得每个epoch打乱顺序是不一致的</p><p>关于梯度回传和参数更新，跟正常情况无异</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
</pre><td class="rouge-code"><pre><span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
    <span class="c1"># record epochs
</span>    <span class="n">train_dataloader</span><span class="p">.</span><span class="n">sampler</span><span class="p">.</span><span class="n">set_epoch</span><span class="p">(</span><span class="n">epoch</span><span class="p">)</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fct</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
    <span class="n">loss</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="p">.</span><span class="n">zero_grad</span><span class="p">()</span>
</pre></table></code></div></div><p>这里有一点需要小心，这个loss是各个进程的loss之和，如果想要存储每个step平均损失，可以进行all_reduce操作，进行平均，不妨看官方的小例子来理解下：</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
</pre><td class="rouge-code"><pre><span class="o">&gt;&gt;&gt;</span> <span class="c1"># All tensors below are of torch.int64 type.
</span><span class="o">&gt;&gt;&gt;</span> <span class="c1"># We have 2 process groups, 2 ranks.
</span><span class="o">&gt;&gt;&gt;</span> <span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">int64</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">rank</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">tensor</span>
<span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span> <span class="c1"># Rank 0
</span><span class="n">tensor</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span> <span class="c1"># Rank 1
</span><span class="o">&gt;&gt;&gt;</span> <span class="n">dist</span><span class="p">.</span><span class="n">all_reduce</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">op</span><span class="o">=</span><span class="n">ReduceOp</span><span class="p">.</span><span class="n">SUM</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">tensor</span>
<span class="n">tensor</span><span class="p">([</span><span class="mi">4</span><span class="p">,</span> <span class="mi">6</span><span class="p">])</span> <span class="c1"># Rank 0
</span><span class="n">tensor</span><span class="p">([</span><span class="mi">4</span><span class="p">,</span> <span class="mi">6</span><span class="p">])</span> <span class="c1"># Rank 1
</span></pre></table></code></div></div><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
</pre><td class="rouge-code"><pre><span class="o">@</span><span class="n">torch</span><span class="p">.</span><span class="n">no_grad</span><span class="p">()</span>
<span class="k">def</span> <span class="nf">reduce_value</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>
    <span class="n">world_size</span> <span class="o">=</span> <span class="n">get_world_size</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">world_size</span> <span class="o">&lt;</span> <span class="mi">2</span><span class="p">:</span>  <span class="c1"># 单GPU的情况
</span>        <span class="k">return</span> <span class="n">value</span>
    <span class="n">dist</span><span class="p">.</span><span class="n">all_reduce</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">average</span><span class="p">:</span>
	    <span class="n">value</span> <span class="o">/=</span> <span class="n">world_size</span>
    <span class="k">return</span> <span class="n">value</span>
</pre></table></code></div></div><p>看到这，肯定有小伙伴要问，那这样我们是不是得先求平均损失再回传梯度啊，不用，因为，当我们回传loss后，DDP会自动对所有<code class="language-plaintext highlighter-rouge">梯度进行平均</code><sup id="fnref:10" role="doc-noteref"><a href="#fn:10" class="footnote" rel="footnote">10</a></sup>，也就是说回传后我们更新的梯度和DP或者单卡同样batch训练都是一致的</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
</pre><td class="rouge-code"><pre><span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fct</span><span class="p">(...)</span>
<span class="n">loss</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>
<span class="c1"># 注意在backward后面
</span><span class="n">loss</span> <span class="o">=</span> <span class="n">reduce_value</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">world_size</span><span class="p">)</span>
<span class="n">mean_loss</span> <span class="o">=</span> <span class="p">(</span><span class="n">step</span> <span class="o">*</span> <span class="n">mean_loss</span> <span class="o">+</span> <span class="n">loss</span><span class="p">.</span><span class="n">item</span><span class="p">())</span> <span class="o">/</span> <span class="p">(</span><span class="n">step</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
</pre></table></code></div></div><p>还有个注意点就是学习率的变化，这个是和batch size息息相关的，如果batch扩充了几倍，也就是说step比之前少了很多，还采用同一个学习率，肯定会出问题的，这里，我们进行线性增大<sup id="fnref:11" role="doc-noteref"><a href="#fn:11" class="footnote" rel="footnote">11</a></sup></p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre><td class="rouge-code"><pre><span class="n">N</span> <span class="o">=</span> <span class="n">world_size</span>
<span class="n">lr</span> <span class="o">=</span> <span class="n">args</span><span class="p">.</span><span class="n">lr</span> <span class="o">*</span> <span class="n">N</span>
</pre></table></code></div></div><p>肯定有人说，诶，你线性增大肯定不能保证梯度的variance一致了，正确的应该是正比于$\sqrt{N}$，关于这个的讨论不妨参考<sup id="fnref:12" role="doc-noteref"><a href="#fn:12" class="footnote" rel="footnote">12</a></sup></p><h4 id="evaluate相关"><span class="mr-2">evaluate相关</span><a href="#evaluate相关" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4><p>接下来，细心的同学肯定好奇了，如果验证集也切分了，metric怎么计算呢？此时就需要咱们把每个进程得到的预测情况集合起来，t就是一个我们需要gather的张量，最后将每个进程中的t按照第一维度拼接，先看官方小例子来理解all_gather</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
</pre><td class="rouge-code"><pre><span class="o">&gt;&gt;&gt;</span> <span class="c1"># All tensors below are of torch.int64 dtype.
</span><span class="o">&gt;&gt;&gt;</span> <span class="c1"># We have 2 process groups, 2 ranks.
</span><span class="o">&gt;&gt;&gt;</span> <span class="n">tensor_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">int64</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">)]</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">tensor_list</span>
<span class="p">[</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]),</span> <span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">])]</span> <span class="c1"># Rank 0 and 1
</span><span class="o">&gt;&gt;&gt;</span> <span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">int64</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">rank</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">tensor</span>
<span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span> <span class="c1"># Rank 0
</span><span class="n">tensor</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span> <span class="c1"># Rank 1
</span><span class="o">&gt;&gt;&gt;</span> <span class="n">dist</span><span class="p">.</span><span class="n">all_gather</span><span class="p">(</span><span class="n">tensor_list</span><span class="p">,</span> <span class="n">tensor</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">tensor_list</span>
<span class="p">[</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]),</span> <span class="n">tensor</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">])]</span> <span class="c1"># Rank 0
</span><span class="p">[</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]),</span> <span class="n">tensor</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">])]</span> <span class="c1"># Rank 1
</span></pre></table></code></div></div><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
</pre><td class="rouge-code"><pre><span class="k">def</span> <span class="nf">sync_across_gpus</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">world_size</span><span class="p">):</span>
    <span class="n">gather_t_tensor</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> 
                       <span class="nb">range</span><span class="p">(</span><span class="n">world_size</span><span class="p">)]</span>
    <span class="n">dist</span><span class="p">.</span><span class="n">all_gather</span><span class="p">(</span><span class="n">gather_t_tensor</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="n">cat</span><span class="p">(</span><span class="n">gather_t_tensor</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></table></code></div></div><p>可以简单参考我前面提供的源码的evaluate部分，我们首先将预测和标签比对，把结果为bool的张量存储下来，最终gather求和取平均。</p><p>这里还有个有趣的地方，tensor默认的类型可能是int，bool型的res拼接后自动转为0和1了，另外bool型的张量是不支持gather的</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
</pre><td class="rouge-code"><pre><span class="k">def</span> <span class="nf">eval</span><span class="p">(...)</span>
    <span class="n">results</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([]).</span><span class="n">cuda</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">step</span><span class="p">,</span> <span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">dataloader</span><span class="p">):</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
        <span class="n">res</span> <span class="o">=</span> <span class="p">(</span><span class="n">outputs</span><span class="p">.</span><span class="n">argmax</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">==</span> <span class="n">labels</span><span class="p">)</span>
        <span class="n">results</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cat</span><span class="p">([</span><span class="n">results</span><span class="p">,</span> <span class="n">res</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

    <span class="n">results</span> <span class="o">=</span> <span class="n">sync_across_gpus</span><span class="p">(</span><span class="n">results</span><span class="p">,</span> <span class="n">world_size</span><span class="p">)</span>
    <span class="n">mean_acc</span> <span class="o">=</span> <span class="p">(</span><span class="n">results</span><span class="p">.</span><span class="nb">sum</span><span class="p">()</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">results</span><span class="p">)).</span><span class="n">item</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">mean_acc</span>
</pre></table></code></div></div><h4 id="模型保存与加载"><span class="mr-2">模型保存与加载</span><a href="#模型保存与加载" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4><p>模型保存，参考部分官方教程<sup id="fnref:13" role="doc-noteref"><a href="#fn:13" class="footnote" rel="footnote">13</a></sup>，我们只需要在主进程保存模型即可，注意，这里是被DDP包裹后的，DDP并没有state_dict，这里barrier的目的是为了让其他进程等待主进程保存模型，以防不同步</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
</pre><td class="rouge-code"><pre><span class="k">def</span> <span class="nf">save_checkpoint</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">path</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">is_main_process</span><span class="p">(</span><span class="n">rank</span><span class="p">):</span>
    	<span class="c1"># All processes should see same parameters as they all 
</span>        <span class="c1"># start from same random parameters and gradients are 
</span>        <span class="c1"># synchronized in backward passes.
</span>        <span class="c1"># Therefore, saving it in one process is sufficient.
</span>        <span class="n">torch</span><span class="p">.</span><span class="n">save</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="n">module</span><span class="p">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="n">path</span><span class="p">)</span>
    
    <span class="c1"># Use a barrier() to keep process 1 waiting for process 0
</span>    <span class="n">dist</span><span class="p">.</span><span class="n">barrier</span><span class="p">()</span>
</pre></table></code></div></div><p>加载的时候别忘了map_location，我们一开始会保存模型至主进程，这样就会导致<code class="language-plaintext highlighter-rouge">cuda:0</code>显存被占据，我们需要将模型remap到其他设备</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
</pre><td class="rouge-code"><pre><span class="k">def</span> <span class="nf">load_checkpoint</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">path</span><span class="p">):</span>
    <span class="c1"># remap the model from cuda:0 to other devices
</span>    <span class="n">map_location</span> <span class="o">=</span> <span class="p">{</span><span class="s">'cuda:%d'</span> <span class="o">%</span> <span class="mi">0</span><span class="p">:</span> <span class="s">'cuda:%d'</span> <span class="o">%</span> <span class="n">rank</span><span class="p">}</span>
    <span class="n">model</span><span class="p">.</span><span class="n">module</span><span class="p">.</span><span class="n">load_state_dict</span><span class="p">(</span>
        <span class="n">torch</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">map_location</span><span class="o">=</span><span class="n">map_location</span><span class="p">)</span>
    <span class="p">)</span>
</pre></table></code></div></div><h4 id="进程销毁"><span class="mr-2">进程销毁</span><a href="#进程销毁" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4><p>运行结束后记得销毁进程：</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
</pre><td class="rouge-code"><pre><span class="k">def</span> <span class="nf">cleanup</span><span class="p">():</span>
    <span class="n">dist</span><span class="p">.</span><span class="n">destroy_process_group</span><span class="p">()</span>
    
<span class="n">cleanup</span><span class="p">()</span>
</pre></table></code></div></div><h4 id="如何启动"><span class="mr-2">如何启动</span><a href="#如何启动" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4><p>在终端输入下列命令【单机多卡】</p><div class="language-bash highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
</pre><td class="rouge-code"><pre>python <span class="nt">-m</span> torch.distributed.launch <span class="nt">--nproc_per_node</span><span class="o">=</span>NUM_GPUS
           main.py <span class="o">(</span><span class="nt">--arg1</span> <span class="nt">--arg2</span> <span class="nt">--arg3</span> and all other
           arguments of your training script<span class="o">)</span>
</pre></table></code></div></div><p>目前<code class="language-plaintext highlighter-rouge">torch 1.10</code>以后更推荐用run</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre><span class="n">torch</span><span class="p">.</span><span class="n">distributed</span><span class="p">.</span><span class="n">launch</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">distributed</span><span class="p">.</span><span class="n">run</span> <span class="o">/</span> <span class="n">torchrun</span>
</pre></table></code></div></div><p>多机多卡是这样的：</p><div class="language-bash highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
</pre><td class="rouge-code"><pre><span class="c"># 第一个节点启动</span>
python <span class="nt">-m</span> torch.distributed.launch <span class="se">\</span>
    <span class="nt">--nproc_per_node</span><span class="o">=</span>NUM_GPUS <span class="se">\</span>
    <span class="nt">--nnodes</span><span class="o">=</span>2 <span class="se">\</span>
    <span class="nt">--node_rank</span><span class="o">=</span>0 <span class="se">\</span>
    <span class="nt">--master_addr</span><span class="o">=</span><span class="s2">"192.168.1.1"</span> <span class="se">\</span>
    <span class="nt">--master_port</span><span class="o">=</span>1234 main.py 

<span class="c"># 第二个节点启动</span>
python <span class="nt">-m</span> torch.distributed.launch <span class="se">\</span>
    <span class="nt">--nproc_per_node</span><span class="o">=</span>NUM_GPUS <span class="se">\</span>
    <span class="nt">--nnodes</span><span class="o">=</span>2 <span class="se">\</span>
    <span class="nt">--node_rank</span><span class="o">=</span>1 <span class="se">\</span>
    <span class="nt">--master_addr</span><span class="o">=</span><span class="s2">"192.168.1.1"</span> <span class="se">\</span>
    <span class="nt">--master_port</span><span class="o">=</span>1234 main.py 
</pre></table></code></div></div><h3 id="mp方法"><span class="mr-2">mp方法</span><a href="#mp方法" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>第二个方法就是利用torch的多线程包</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
</pre><td class="rouge-code"><pre><span class="kn">import</span> <span class="nn">torch.multiprocessing</span> <span class="k">as</span> <span class="n">mp</span>
<span class="c1"># rank mp会自动填入
</span><span class="k">def</span> <span class="nf">main</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">arg1</span><span class="p">,</span> <span class="p">...):</span>
    <span class="k">pass</span>

<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="s">'__main__'</span><span class="p">:</span>
    <span class="n">mp</span><span class="p">.</span><span class="n">spawn</span><span class="p">(</span><span class="n">main</span><span class="p">,</span> <span class="n">nprocs</span><span class="o">=</span><span class="n">TOTAL_GPUS</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">arg1</span><span class="p">,</span> <span class="p">...))</span>
</pre></table></code></div></div><p>这种运行的时候就跟正常的python文件一致：</p><div class="language-bash highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre>python main.py
</pre></table></code></div></div><h2 id="优缺点-1"><span class="mr-2">优缺点</span><a href="#优缺点-1" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><ul><li><strong>优点</strong>： 相比于DP而言，不需要反复创建和销毁线程；Ring-AllReduce算法提高通信效率；模型同步方便<li><strong>缺点</strong>：操作起来可能有些复杂，一般可满足需求的可先试试看DataParallel</ul><h1 id="references">References</h1><div class="footnotes" role="doc-endnotes"><ol><li id="fn:1" role="doc-endnote"><p>https://blog.csdn.net/qq_37541097/article/details/109736159 <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p><li id="fn:2" role="doc-endnote"><p>https://www.cnblogs.com/ljwgis/p/15471530.html <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p><li id="fn:3" role="doc-endnote"><p>https://pytorch.org/tutorials/beginner/former_torchies/parallelism_tutorial.html?highlight=dataparallel <a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a></p><li id="fn:4" role="doc-endnote"><p>https://pytorch.org/tutorials/beginner/blitz/data_parallel_tutorial.html <a href="#fnref:4" class="reversefootnote" role="doc-backlink">&#8617;</a></p><li id="fn:5" role="doc-endnote"><p>https://github.com/kimiyoung/transformer-xl <a href="#fnref:5" class="reversefootnote" role="doc-backlink">&#8617;</a></p><li id="fn:6" role="doc-endnote"><p>https://github.com/sherlcok314159/dl-tools/blob/main/balanced_data_parallel/README.md <a href="#fnref:6" class="reversefootnote" role="doc-backlink">&#8617;</a></p><li id="fn:7" role="doc-endnote"><p>https://www.youtube.com/watch?v=rj-hjS5L8Bw <a href="#fnref:7" class="reversefootnote" role="doc-backlink">&#8617;</a></p><li id="fn:8" role="doc-endnote"><p>https://pytorch.org/docs/stable/distributed.html#backends <a href="#fnref:8" class="reversefootnote" role="doc-backlink">&#8617;</a></p><li id="fn:9" role="doc-endnote"><p>https://stackoverflow.com/questions/58271635/in-distributed-computing-what-are-world-size-and-rank <a href="#fnref:9" class="reversefootnote" role="doc-backlink">&#8617;</a></p><li id="fn:10" role="doc-endnote"><p>https://discuss.pytorch.org/t/average-loss-in-dp-and-ddp/93306/4 <a href="#fnref:10" class="reversefootnote" role="doc-backlink">&#8617;</a></p><li id="fn:11" role="doc-endnote"><p>https://arxiv.org/abs/1706.02677 <a href="#fnref:11" class="reversefootnote" role="doc-backlink">&#8617;</a></p><li id="fn:12" role="doc-endnote"><p>https://github.com/Lightning-AI/lightning/discussions/3706 <a href="#fnref:12" class="reversefootnote" role="doc-backlink">&#8617;</a></p><li id="fn:13" role="doc-endnote"><p>https://pytorch.org/tutorials/intermediate/ddp_tutorial.html <a href="#fnref:13" class="reversefootnote" role="doc-backlink">&#8617;</a></p></ol></div><hr><h4>Sponsoring</h4><p>If you think of something really helpful, please consider sponsoring me! Any support is well appreciated! &#128516;&#128516;&#128516;</p><a href="https://ko-fi.com/yunpengtai"><img data-src="https://img.shields.io/badge/-Buy%20Me%20a%20Coffee-ff5f5f?logo=ko-fi&amp;logoColor=https://s2.loli.net/white" data-proofer-ignore></a> <a href="https://s2.loli.net/2022/06/19/wfh2rnYBq8vzZuD.jpg"><img data-src="https://img.shields.io/badge/-Tip%20Me%20on%20WeChat-brightgreen?logo=wechat&logoColor=https://s2.loli.net/white" data-proofer-ignore> </a> <a href="https://s2.loli.net/2022/06/19/kxm7wo8f3zTrcWt.jpg"><img data-src="https://img.shields.io/badge/-Tip%20Me%20on%20Alipay-blue?logo=alipay&logoColor=https://s2.loli.net/white" data-proofer-ignore></a></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/training/'>Training</a>, <a href='/categories/pytorch/'>PyTorch</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/multi-gpus/" class="post-tag no-text-decoration" >Multi-gpus</a> <a href="/tags/dataparallel/" class="post-tag no-text-decoration" >DataParallel</a> <a href="/tags/distributed-training/" class="post-tag no-text-decoration" >Distributed Training</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=Diving+in+distributed+training+in+PyTorch+-+Yunpeng+Tai&url=https%3A%2F%2Fwww.yunpengtai.top%2Fposts%2FDiving-in-distributed-training%2F" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=Diving+in+distributed+training+in+PyTorch+-+Yunpeng+Tai&u=https%3A%2F%2Fwww.yunpengtai.top%2Fposts%2FDiving-in-distributed-training%2F" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://t.me/share/url?url=https%3A%2F%2Fwww.yunpengtai.top%2Fposts%2FDiving-in-distributed-training%2F&text=Diving+in+distributed+training+in+PyTorch+-+Yunpeng+Tai" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <i id="copy-link" class="fa-fw fas fa-link small" data-toggle="tooltip" data-placement="top" title="Copy link" data-title-succeed="Link copied successfully!"> </i> </span></div></div></div></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted"><div class="access"><div id="access-lastmod" class="post"><div class="panel-heading">Recently Updated</div><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/posts/Back-propagation/">Going Deeper into Back-propagation</a><li><a href="/posts/Diving-in-distributed-training/">Diving in distributed training in PyTorch</a><li><a href="/posts/hello-world/">Hello World</a><li><a href="/posts/Tips-for-Training-Neural-Networks/">Tips for Training Neural Networks</a><li><a href="/posts/Quotes-of-Mathematicians/">Quotes of Mathematicians</a></ul></div><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/1st-post/">1st post</a> <a class="post-tag" href="/tags/dataparallel/">DataParallel</a> <a class="post-tag" href="/tags/distributed-training/">Distributed Training</a> <a class="post-tag" href="/tags/experience/">Experience</a> <a class="post-tag" href="/tags/gradients/">Gradients</a> <a class="post-tag" href="/tags/hello-world/">hello world</a> <a class="post-tag" href="/tags/mathematics/">Mathematics</a> <a class="post-tag" href="/tags/multi-gpus/">Multi-gpus</a> <a class="post-tag" href="/tags/neural-networks/">Neural Networks</a> <a class="post-tag" href="/tags/optimization/">Optimization</a></div></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"><div class="panel-heading pl-3 pt-2 mb-2">Contents</div><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div id="tail-wrapper" class="col-12 col-lg-11 col-xl-9 pl-3 pr-3 pr-xl-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/Tips-for-Training-Neural-Networks/"><div class="card-body"> <em class="small" data-ts="1659181380" data-df="ll" > Jul 30, 2022 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Tips for Training Neural Networks</h3><div class="text-muted small"><p> Recently, I have read a blog about training neural networks (simplified as NN in the rest part of this post) and it is really amazing. I am going to add my own experience in this post along with su...</p></div></div></a></div><div class="card"> <a href="/posts/Back-propagation/"><div class="card-body"> <em class="small" data-ts="1662520200" data-df="ll" > Sep 7, 2022 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Going Deeper into Back-propagation</h3><div class="text-muted small"><p> 1. Gradient descent optimization Gradient-based methods make use of the gradient information to adjust the parameters. Among them, gradient descent can be the simplest. Gradient descent makes the ...</p></div></div></a></div><div class="card"> <a href="/posts/Quotes-of-Mathematicians/"><div class="card-body"> <em class="small" data-ts="1658541360" data-df="ll" > Jul 23, 2022 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Quotes of Mathematicians</h3><div class="text-muted small"><p> Life is complex, and it has both real and imaginary parts. — Someone Basically, I’m not interested in doing research and I never have been… I’m interested in understanding, which is quite a di...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/posts/Back-propagation/" class="btn btn-outline-primary" prompt="Older"><p>Going Deeper into Back-propagation</p></a><div class="btn btn-outline-primary disabled" prompt="Newer"><p>-</p></div></div><script src="https://unpkg.com/@waline/client@v2/dist/waline.js"></script><link rel="stylesheet" href="https://unpkg.com/@waline/client@v2/dist/waline.css" /><div id="waline"></div><script> $(function() { const locale = { placeholder: '可以匿名评论哦~ QQ邮箱可自动获取头像 Anything to say? It can be anonymous.', level0: '潜水', level1: '冒泡', level2: '摸鱼', level3: '话痨', level4: '话满天', level5: '龙王', }; Waline.init({ el: '#waline', lang: 'en', reaction: true, serverURL: 'https://example.yunpengtai.top/', emoji: [ 'https://gcore.jsdelivr.net/gh/norevi/waline-blobcatemojis@1.0/blobs', 'https://unpkg.com/@waline/emojis@1.1.0/tw-emoji', 'https://unpkg.com/@waline/emojis@1.1.0/bmoji' ], reaction:[ 'https://gcore.jsdelivr.net/gh/norevi/waline-blobcatemojis@1.0/blobs/ablobcatheart.png', 'https://gcore.jsdelivr.net/gh/norevi/waline-blobcatemojis@1.0/blobs/ablobcatrainbow.png', 'https://gcore.jsdelivr.net/gh/norevi/waline-blobcatemojis@1.0/blobs/ablobcatattentionreverse.png', 'https://gcore.jsdelivr.net/gh/norevi/waline-blobcatemojis@1.0/blobs/ablobcatheartbroken.png', 'https://gcore.jsdelivr.net/gh/norevi/waline-blobcatemojis@1.0/blobs/blobcatopenmouth.png', 'https://gcore.jsdelivr.net/gh/norevi/waline-blobcatemojis@1.0/blobs/blobcatcoffee.png' ], dark: "__waline__css__", locale, }); let head = document.getElementsByTagName("head")[0]; let css = head.lastChild; let cssContent = css.textContent.replace("__waline__css__", ""); let cssContentPerferredDark = "@media (prefers-color-scheme: dark){html:not([data-mode])" + cssContent + "}"; let cssContentSelectedDark = "html[data-mode=dark]" + cssContent; css.textContent = cssContentPerferredDark; let style = document.createElement('style'); style.textContent = cssContentSelectedDark; head.appendChild(style); }); </script></div></div><footer class="row pl-3 pr-3"><div class="col-12 d-flex justify-content-between align-items-center text-muted pl-0 pr-0"><div class="footer-left"><p class="mb-0"> © 2023 <a href="https://twitter.com/TonySta14611077">Yunpeng Tai</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/1st-post/">1st post</a> <a class="post-tag" href="/tags/dataparallel/">DataParallel</a> <a class="post-tag" href="/tags/distributed-training/">Distributed Training</a> <a class="post-tag" href="/tags/experience/">Experience</a> <a class="post-tag" href="/tags/gradients/">Gradients</a> <a class="post-tag" href="/tags/hello-world/">hello world</a> <a class="post-tag" href="/tags/mathematics/">Mathematics</a> <a class="post-tag" href="/tags/multi-gpus/">Multi-gpus</a> <a class="post-tag" href="/tags/neural-networks/">Neural Networks</a> <a class="post-tag" href="/tags/optimization/">Optimization</a></div></div></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a><div id="notification" class="toast" role="alert" aria-live="assertive" aria-atomic="true" data-animation="true" data-autohide="false"><div class="toast-header"> <button type="button" class="ml-2 ml-auto close" data-dismiss="toast" aria-label="Close"> <span aria-hidden="true">&times;</span> </button></div><div class="toast-body text-center pt-0"><p class="pl-2 pr-2 mb-3">A new version of content is available.</p><button type="button" class="btn btn-primary" aria-label="Update"> Update </button></div></div><script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No results found.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script> <script src="https://cdn.jsdelivr.net/combine/npm/magnific-popup@1/dist/jquery.magnific-popup.min.js,npm/lozad/dist/lozad.min.js,npm/clipboard@2/dist/clipboard.min.js"></script> <script src="https://cdn.jsdelivr.net/combine/npm/dayjs@1/dayjs.min.js,npm/dayjs@1/locale/en.min.js,npm/dayjs@1/plugin/relativeTime.min.js,npm/dayjs@1/plugin/localizedFormat.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script> /* see: <https://docs.mathjax.org/en/latest/options/input/tex.html#tex-options> */ MathJax = { tex: { inlineMath: [ /* start/end delimiter pairs for in-line math */ ['$','$'], ['\\(','\\)'] ], displayMath: [ /* start/end delimiter pairs for display math */ ['$$', '$$'], ['\\[', '\\]'] ] } }; </script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"> </script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/js/bootstrap.bundle.min.js"></script> <script defer src="/app.js"></script> <script defer src="https://www.googletagmanager.com/gtag/js?id=G-Y3CX2RWEDY"></script> <script> document.addEventListener("DOMContentLoaded", function(event) { window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'G-Y3CX2RWEDY'); }); </script>
