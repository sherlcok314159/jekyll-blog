<!DOCTYPE html><html lang="en" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="generator" content="Jekyll v4.2.2" /><meta property="og:title" content="Retrieval-Enhanced Transformer" /><meta property="og:locale" content="en" /><meta name="description" content="Problems To Solve To Scale Down the model size while maintaining the performances. To incorporate External Memory Retrieval in the Large Language Model Modeling." /><meta property="og:description" content="Problems To Solve To Scale Down the model size while maintaining the performances. To incorporate External Memory Retrieval in the Large Language Model Modeling." /><link rel="canonical" href="https://www.yunpengtai.top/posts/Retieval-Enhanced-transformer/" /><meta property="og:url" content="https://www.yunpengtai.top/posts/Retieval-Enhanced-transformer/" /><meta property="og:site_name" content="Yunpeng Tai" /><meta property="og:image" content="https://s2.loli.net//2022/06/19/Xvif95kmEYgHupM.jpg" /><meta property="og:image:height" content="100" /><meta property="og:image:width" content="600" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2022-06-19T21:06:00+08:00" /><meta name="twitter:card" content="summary_large_image" /><meta property="twitter:image" content="https://s2.loli.net//2022/06/19/Xvif95kmEYgHupM.jpg" /><meta property="twitter:title" content="Retrieval-Enhanced Transformer" /><meta name="twitter:site" content="@TonySta14611077" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2022-07-22T14:22:13+08:00","datePublished":"2022-06-19T21:06:00+08:00","description":"Problems To Solve To Scale Down the model size while maintaining the performances. To incorporate External Memory Retrieval in the Large Language Model Modeling.","headline":"Retrieval-Enhanced Transformer","image":{"width":600,"height":100,"url":"https://www.yunpengtai.top/2022/06/19/Xvif95kmEYgHupM.jpg","@type":"imageObject"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://www.yunpengtai.top/posts/Retieval-Enhanced-transformer/"},"url":"https://www.yunpengtai.top/posts/Retieval-Enhanced-transformer/"}</script><title>Retrieval-Enhanced Transformer | Yunpeng Tai</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="Yunpeng Tai"><meta name="application-name" content="Yunpeng Tai"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin><link rel="dns-prefetch" href="https://fonts.gstatic.com" crossorigin><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://cdn.jsdelivr.net" ><link rel="dns-prefetch" href="https://cdn.jsdelivr.net" ><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Lato&family=Source+Sans+Pro:wght@400;600;700;900&display=swap"><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://www.google-analytics.com"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://www.googletagmanager.com"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1/dist/magnific-popup.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script> <script type="text/javascript"> class ModeToggle { static get MODE_KEY() { return "mode"; } static get MODE_ATTR() { return "data-mode"; } static get DARK_MODE() { return "dark"; } static get LIGHT_MODE() { return "light"; } static get ID() { return "mode-toggle"; } constructor() { if (this.hasMode) { if (this.isDarkMode) { if (!this.isSysDarkPrefer) { this.setDark(); } } else { if (this.isSysDarkPrefer) { this.setLight(); } } } let self = this; /* always follow the system prefers */ this.sysDarkPrefers.addEventListener("change", () => { if (self.hasMode) { if (self.isDarkMode) { if (!self.isSysDarkPrefer) { self.setDark(); } } else { if (self.isSysDarkPrefer) { self.setLight(); } } self.clearMode(); } self.notify(); }); } /* constructor() */ get sysDarkPrefers() { return window.matchMedia("(prefers-color-scheme: dark)"); } get isSysDarkPrefer() { return this.sysDarkPrefers.matches; } get isDarkMode() { return this.mode === ModeToggle.DARK_MODE; } get isLightMode() { return this.mode === ModeToggle.LIGHT_MODE; } get hasMode() { return this.mode != null; } get mode() { return sessionStorage.getItem(ModeToggle.MODE_KEY); } /* get the current mode on screen */ get modeStatus() { if (this.isDarkMode || (!this.hasMode && this.isSysDarkPrefer)) { return ModeToggle.DARK_MODE; } else { return ModeToggle.LIGHT_MODE; } } setDark() { $('html').attr(ModeToggle.MODE_ATTR, ModeToggle.DARK_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); } setLight() { $('html').attr(ModeToggle.MODE_ATTR, ModeToggle.LIGHT_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); } clearMode() { $('html').removeAttr(ModeToggle.MODE_ATTR); sessionStorage.removeItem(ModeToggle.MODE_KEY); } /* Notify another plugins that the theme mode has changed */ notify() { window.postMessage({ direction: ModeToggle.ID, message: this.modeStatus }, "*"); } } /* ModeToggle */ const toggle = new ModeToggle(); function flipMode() { if (toggle.hasMode) { if (toggle.isSysDarkPrefer) { if (toggle.isLightMode) { toggle.clearMode(); } else { toggle.setLight(); } } else { if (toggle.isDarkMode) { toggle.clearMode(); } else { toggle.setDark(); } } } else { if (toggle.isSysDarkPrefer) { toggle.setLight(); } else { toggle.setDark(); } } toggle.notify(); } /* flipMode() */ </script><body data-spy="scroll" data-target="#toc" data-topbar-visible="true"><div id="sidebar" class="d-flex flex-column align-items-end"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" class="mx-auto"> <img src="https://avatars.githubusercontent.com/u/76043326?s=400&u=1f9b3c508db6b5caec736e7c1edb42ba76e2f2a1&v=4" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/">Yunpeng Tai</a></div><div class="site-subtitle font-italic">Learn to think</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tag ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fas fa-info-circle ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center align-items-center"> <button class="mode-toggle btn" aria-label="Switch Mode"> <i class="fas fa-adjust"></i> </button> <span class="icon-border"></span> <a href="https://github.com/sherlcok314159" aria-label="github" target="_blank" rel="noopener"> <i class="fab fa-github"></i> </a> <a href="https://twitter.com/TonySta14611077" aria-label="twitter" target="_blank" rel="noopener"> <i class="fab fa-twitter"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['yunpengtai.typ','gmail.com'].join('@')" aria-label="email" > <i class="fas fa-envelope"></i> </a></div></div><div id="topbar-wrapper"><div id="topbar" class="container d-flex align-items-center justify-content-between h-100 pl-3 pr-3 pl-md-4 pr-md-4"> <span id="breadcrumb"> <span> <a href="/"> Home </a> </span> <span>Retrieval-Enhanced Transformer</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..."> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper" class="d-flex justify-content-center"><div id="main" class="container pl-xl-4 pr-xl-4"><div class="row"><div id="core-wrapper" class="col-12 col-lg-11 col-xl-9 pr-xl-4"><div class="post pl-1 pr-1 pl-md-2 pr-md-2"><h1 data-toc-skip>Retrieval-Enhanced Transformer</h1><div class="post-meta text-muted"> <span> Posted <em class="" data-ts="1655643960" data-df="ll" data-toggle="tooltip" data-placement="bottom"> Jun 19, 2022 </em> </span> <span> Updated <em class="" data-ts="1658470933" data-df="ll" data-toggle="tooltip" data-placement="bottom"> Jul 22, 2022 </em> </span><div class="mt-3 mb-3"> <img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 600 100'%3E%3C/svg%3E" data-src="https://s2.loli.net//2022/06/19/Xvif95kmEYgHupM.jpg" class="preview-img bg" alt="Preview Image" width="600" height="100" data-proofer-ignore></div><div class="d-flex justify-content-between"> <span></span><div> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="722 words"> <em>4 min</em> read</span></div></div></div><div class="post-content"><h2 id="problems-to-solve"><span class="mr-2">Problems To Solve</span><a href="#problems-to-solve" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><ol><li>To <b>Scale Down</b> the model size while maintaining the performances.<li>To incorporate <code class="language-plaintext highlighter-rouge">External Memory Retrieval</code> in the Large Language Model Modeling.</ol><p><img data-src="https://s2.loli.net/2022/06/19/ecSmGwTuBbzYnDX.png" alt="" data-proofer-ignore></p><h2 id="how"><span class="mr-2">How?</span><a href="#how" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><h3 id="data-construction"><span class="mr-2">Data Construction</span><a href="#data-construction" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><ol><li>Training &amp; Evaluation set:<ol><li>$\text{MassiveText}$ for both training &amp; retrieval data (contains 5 trillion tokens) <img data-src="https://s2.loli.net/2022/06/19/JUpDF8y9LCqnRNW.png" alt="" data-proofer-ignore><li><code class="language-plaintext highlighter-rouge">SentencePiece</code> with a vocabulary of $128K$ tokens<li>During training, we retrieving $600B$ tokens from the training<li>The evaluation contains $1.75T$ tokens</ol><li><code class="language-plaintext highlighter-rouge">Test set</code> leakage: Due to the huge retrieving database, the test set may have appeared in the training set. Thus, the authors apply <code class="language-plaintext highlighter-rouge">13-gram Jaccard Similarity</code> between the training and test documents to <em>filter</em> those training documents similar to the test documents (i.e., the similarity is $\geq \textbf{0.80}$)</ol><h3 id="retrieval-modeling"><span class="mr-2">Retrieval Modeling</span><a href="#retrieval-modeling" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><ol><li><code class="language-plaintext highlighter-rouge">Key-Value</code> Format of the Database:<ol><li>$\text{Key} \Rightarrow$ <code class="language-plaintext highlighter-rouge">frozen</code> BERT Embedding<li>$\text{Value} \Rightarrow$ raw <code class="language-plaintext highlighter-rouge">chunks</code> of the tokens</ol><li>using the <code class="language-plaintext highlighter-rouge">SCaNN</code> library<li><p>the similarity depends on the $\text{L2 Distance}$:</p>\[||x-y||_2 = \sqrt{\sum_i (x_i - y_i)^2}\]<li><strong>pre-compute</strong> the <code class="language-plaintext highlighter-rouge">frozen</code> BERT Embedding to save the computation and the Embedding is <strong>averaged with time.</strong><li>retrieving targets are the corresponding chunks and their <code class="language-plaintext highlighter-rouge">continuation</code> in the orig document</ol><h3 id="the-whole-architecture"><span class="mr-2">The whole architecture</span><a href="#the-whole-architecture" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><h4 id="the-pipeline"><span class="mr-2">The pipeline</span><a href="#the-pipeline" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4><p><img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 500 800'%3E%3C/svg%3E" data-src="https://s2.loli.net/2022/06/19/SMKJbATvzyqRE3c.png" alt="" width="500" height="800" data-proofer-ignore></p><ol><li>Assume the input sequence $\text{X}$ contains $9$ tokens, it can be <code class="language-plaintext highlighter-rouge">split</code> into $3$ chunks (i.e., $C_1, C_2, C_3$) whose sizes are $3$ respectively.<li>Then the chunks are embedded through the frozen BERT embedding. We can retrieve neighbours of those input chunks.<li>We also embed the input sequence and then apply <code class="language-plaintext highlighter-rouge">self-attention mechanism</code> on them to get the hidden states $H(X)$<li>Furthermore, we need to encode the neighbours. Here, the transformer encoder is <code class="language-plaintext highlighter-rouge">bi-directional</code>. And it outputs the representations of the neighbours by <code class="language-plaintext highlighter-rouge">conditioning</code> on the hidden states of the input chunks.<li>After we get the representations of the neighbours, we let them <code class="language-plaintext highlighter-rouge">attend</code> the input chunks as the $\text{K and V}$ while the input chunk is $\text{Q}$. The attending network is called CCA($\textbf{C}$hunked $\textbf{C}$ross $\textbf{A}$ttention). I introduce it in the following part.<li>When the neighbours finish attending the input chunks, the input chunks can be <code class="language-plaintext highlighter-rouge">represented</code> by the retrieved neighbours. The representations are going through the FFW($\textbf{F}$eed $\textbf{F}$or$\textbf{W}$ard). Thus, a <code class="language-plaintext highlighter-rouge">Retro-Block</code> contains self-attention mechanism, CCA and FFW.</ol><h4 id="chunked-cross-attention"><span class="mr-2">Chunked Cross Attention</span><a href="#chunked-cross-attention" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4><p><img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 500 800'%3E%3C/svg%3E" data-src="https://s2.loli.net/2022/06/19/EJ4GsShCHox2dmn.png" alt="" width="500" height="800" data-proofer-ignore></p><ol><li>Take the green chunk as the example, we retrieve its neighbours from the database and we let them attend with the <code class="language-plaintext highlighter-rouge">concatenation</code> between the green chunk and its next chunk. To put it more precisely, assume we retrieve the neighbours $E(m_i)$ for the chunk $m_i$ which contains $n$ tokens: ${m_{i1}, m_{i2}, \dots, m_{in}}$, we concatenate the <code class="language-plaintext highlighter-rouge">last</code> token of $m_i$ with the <code class="language-plaintext highlighter-rouge">next</code> chunk $m_j$ <code class="language-plaintext highlighter-rouge">except</code> the last token $\Rightarrow \text{Concatenate}(m_{in}, m_{j1, \dots, jn-1})$.<li>After the concatenation, we apply <code class="language-plaintext highlighter-rouge">CA</code>($\textbf{C}$ross $\textbf{A}$ttention). CA is the common attention mechanism.<li>Finally, we <code class="language-plaintext highlighter-rouge">concatenate</code> the outputs and <code class="language-plaintext highlighter-rouge">pad</code> them.</ol><blockquote class="prompt-tip"><div><p>Note, the relative positional encoding is applied.</p></div></blockquote><h2 id="experiment"><span class="mr-2">Experiment</span><a href="#experiment" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><h3 id="scaling-the-retro"><span class="mr-2">Scaling the Retro</span><a href="#scaling-the-retro" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p><img data-src="https://s2.loli.net/2022/06/19/2Pl7NrxWuyzkEwO.png" alt="" data-proofer-ignore></p><p><img data-src="https://s2.loli.net/2022/06/19/DzKGVWuH2r4Tvdj.png" alt="" data-proofer-ignore></p><ol><li>The <code class="language-plaintext highlighter-rouge">scale</code> of the Retro and the retrieved <code class="language-plaintext highlighter-rouge">tokens</code> are <code class="language-plaintext highlighter-rouge">proportional</code> to the performance.<li>The number of neighbours has an <code class="language-plaintext highlighter-rouge">upped bound</code>: somewhere near $40$. Maybe too many neighbours <code class="language-plaintext highlighter-rouge">reduce</code> the retrieval quality.</ol><h3 id="improvement-comparison"><span class="mr-2">Improvement Comparison</span><a href="#improvement-comparison" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p><img data-src="https://s2.loli.net/2022/06/19/yeOLafErnChsq3K.png" alt="" data-proofer-ignore></p><ol><li>Among some tasks, Retro can <code class="language-plaintext highlighter-rouge">outperform</code> the models whose parameters are much more than the Retro’s.</ol><h3 id="perplexity-on-wikitext103"><span class="mr-2">Perplexity on Wikitext103</span><a href="#perplexity-on-wikitext103" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p><img data-src="https://s2.loli.net/2022/06/19/Fueq18xZWtO5CwV.png" alt="" data-proofer-ignore></p><ol><li>Retro’s perplexity can be <code class="language-plaintext highlighter-rouge">SOTA</code> on the Wikitext103<li>Interestingly, the external memory can also have the phenomenon of the <code class="language-plaintext highlighter-rouge">underfitting</code>. When using <code class="language-plaintext highlighter-rouge">MassiveText(1%)</code>, it can underfit the training set. And its performance is worse than the <code class="language-plaintext highlighter-rouge">kNN-LM</code>.</ol><h3 id="retro-finetuning"><span class="mr-2">Retro Finetuning</span><a href="#retro-finetuning" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p><img data-src="https://s2.loli.net/2022/06/19/oBCmrJ5XPqZuezL.png" alt="" data-proofer-ignore></p><ol><li><code class="language-plaintext highlighter-rouge">Training from scratch</code> is the most powerful way.</ol><h3 id="question-answering-results"><span class="mr-2">Question Answering Results</span><a href="#question-answering-results" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p><img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 500 400'%3E%3C/svg%3E" data-src="https://s2.loli.net/2022/06/19/9NX3QnemqWwY7oC.png" alt="" width="500" height="400" data-proofer-ignore></p><ol><li><code class="language-plaintext highlighter-rouge">FID + Distill</code> is the <code class="language-plaintext highlighter-rouge">SOTA</code> in the Open-Domain Question Answering when the retrieval involves in the training.</ol><h3 id="ablation-studies"><span class="mr-2">Ablation Studies</span><a href="#ablation-studies" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p><img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 500 800'%3E%3C/svg%3E" data-src="https://s2.loli.net/2022/06/19/WaxCPTmzJSw8dDi.png" alt="" width="500" height="800" data-proofer-ignore></p><ol><li>The <code class="language-plaintext highlighter-rouge">continuation</code> of the retrieved chunks do <code class="language-plaintext highlighter-rouge">help</code>.<li>CA positions are <code class="language-plaintext highlighter-rouge">every 3 from 1 or mid layer</code>.</ol><h2 id="why-work"><span class="mr-2">Why work?</span><a href="#why-work" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><ol><li>To summarize, the Retro incorporates the <code class="language-plaintext highlighter-rouge">external</code> neighbours of the input sequence into the Large Language Modelling to <code class="language-plaintext highlighter-rouge">scale down</code> the model size while <code class="language-plaintext highlighter-rouge">maintaining</code> the performance.</ol><h2 id="lessons--imaginations"><span class="mr-2">Lessons &amp; Imaginations</span><a href="#lessons--imaginations" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><ol><li>Performance can get improved either by improving the <code class="language-plaintext highlighter-rouge">model size</code> or training <code class="language-plaintext highlighter-rouge">more data</code>.<li>Huge amount of data <code class="language-plaintext highlighter-rouge">don't</code> need too <code class="language-plaintext highlighter-rouge">big</code> model to fit in.<li>We can scale down the PLM by attending the <code class="language-plaintext highlighter-rouge">external information</code>.<li>CCA is applied because the external knowledge need to be merged. When applying in <code class="language-plaintext highlighter-rouge">MRC</code>, the <code class="language-plaintext highlighter-rouge">external</code> information can be:<ol><li>the chunked passages<li>the broken passages<li>the past similar to question-passage pairs<li>the knowledge among the input<li>the evidence</ol><li>The <code class="language-plaintext highlighter-rouge">BM25, Edit Distance and LDA</code> can also perform not bad in the retieval.</ol><hr><h4>Sponsoring</h4><p>If you think of something really helpful, please consider sponsoring me! Any support is well appreciated! &#128516;&#128516;&#128516;</p><a href="https://ko-fi.com/yunpengtai"><img data-src="https://img.shields.io/badge/-Buy%20Me%20a%20Coffee-ff5f5f?logo=ko-fi&amp;logoColor=https://s2.loli.net/white" data-proofer-ignore></a> <a href="https://s2.loli.net/2022/06/19/wfh2rnYBq8vzZuD.jpg"><img data-src="https://img.shields.io/badge/-Tip%20Me%20on%20WeChat-brightgreen?logo=wechat&logoColor=https://s2.loli.net/white" data-proofer-ignore> </a> <a href="https://s2.loli.net/2022/06/19/kxm7wo8f3zTrcWt.jpg"><img data-src="https://img.shields.io/badge/-Tip%20Me%20on%20Alipay-blue?logo=alipay&logoColor=https://s2.loli.net/white" data-proofer-ignore></a></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/plms/'>PLMs</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/retrieval/" class="post-tag no-text-decoration" >retrieval</a> <a href="/tags/transformer/" class="post-tag no-text-decoration" >transformer</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=Retrieval-Enhanced+Transformer+-+Yunpeng+Tai&url=https%3A%2F%2Fwww.yunpengtai.top%2Fposts%2FRetieval-Enhanced-transformer%2F" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=Retrieval-Enhanced+Transformer+-+Yunpeng+Tai&u=https%3A%2F%2Fwww.yunpengtai.top%2Fposts%2FRetieval-Enhanced-transformer%2F" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://t.me/share/url?url=https%3A%2F%2Fwww.yunpengtai.top%2Fposts%2FRetieval-Enhanced-transformer%2F&text=Retrieval-Enhanced+Transformer+-+Yunpeng+Tai" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <i id="copy-link" class="fa-fw fas fa-link small" data-toggle="tooltip" data-placement="top" title="Copy link" data-title-succeed="Link copied successfully!"> </i> </span></div></div></div></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted"><div class="access"><div id="access-lastmod" class="post"><div class="panel-heading">Recently Updated</div><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/posts/Back-propagation/">Going Deeper into Back-propagation</a><li><a href="/posts/Diving-in-distributed-training/">Diving in distributed training in PyTorch</a><li><a href="/posts/hello-world/">Hello World</a><li><a href="/posts/Tips-for-Training-Neural-Networks/">Tips for Training Neural Networks</a><li><a href="/posts/Quotes-of-Mathematicians/">Quotes of Mathematicians</a></ul></div><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/1st-post/">1st post</a> <a class="post-tag" href="/tags/dataparallel/">DataParallel</a> <a class="post-tag" href="/tags/distributed-training/">Distributed Training</a> <a class="post-tag" href="/tags/experience/">Experience</a> <a class="post-tag" href="/tags/gradients/">Gradients</a> <a class="post-tag" href="/tags/hello-world/">hello world</a> <a class="post-tag" href="/tags/mathematics/">Mathematics</a> <a class="post-tag" href="/tags/multi-gpus/">Multi-gpus</a> <a class="post-tag" href="/tags/neural-networks/">Neural Networks</a> <a class="post-tag" href="/tags/optimization/">Optimization</a></div></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"><div class="panel-heading pl-3 pt-2 mb-2">Contents</div><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div id="tail-wrapper" class="col-12 col-lg-11 col-xl-9 pl-3 pr-3 pr-xl-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/Diving-in-distributed-training/"><div class="card-body"> <em class="small" data-ts="1668951420" data-df="ll" > Nov 20, 2022 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Diving in distributed training in PyTorch</h3><div class="text-muted small"><p> 鉴于网上此类教程有不少模糊不清，对原理不得其法，代码也难跑通，故而花了几天细究了一下相关原理和实现，欢迎批评指正！ 关于此部分的代码，可以去这里查看 在开始前，我需要特别致谢一下一位挚友，他送了我双显卡的机器来赞助我做个人研究，否则多卡的相关实验就得付费在云平台上跑了，感谢好朋友一路以来的支持，这份恩情值得一辈子铭记！这篇文章作为礼物赠与挚友。 Why Parallel 我们...</p></div></div></a></div><div class="card"> <a href="/posts/Back-propagation/"><div class="card-body"> <em class="small" data-ts="1662520200" data-df="ll" > Sep 7, 2022 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Going Deeper into Back-propagation</h3><div class="text-muted small"><p> 1. Gradient descent optimization Gradient-based methods make use of the gradient information to adjust the parameters. Among them, gradient descent can be the simplest. Gradient descent makes the ...</p></div></div></a></div><div class="card"> <a href="/posts/Tips-for-Training-Neural-Networks/"><div class="card-body"> <em class="small" data-ts="1659181380" data-df="ll" > Jul 30, 2022 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Tips for Training Neural Networks</h3><div class="text-muted small"><p> Recently, I have read a blog about training neural networks (simplified as NN in the rest part of this post) and it is really amazing. I am going to add my own experience in this post along with su...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/posts/hello-world/" class="btn btn-outline-primary" prompt="Older"><p>Hello World</p></a> <a href="/posts/Quotes-of-Mathematicians/" class="btn btn-outline-primary" prompt="Newer"><p>Quotes of Mathematicians</p></a></div><script src="https://unpkg.com/@waline/client@v2/dist/waline.js"></script><link rel="stylesheet" href="https://unpkg.com/@waline/client@v2/dist/waline.css" /><div id="waline"></div><script> $(function() { const locale = { placeholder: '可以匿名评论哦~ QQ邮箱可自动获取头像 Anything to say? It can be anonymous.', level0: '潜水', level1: '冒泡', level2: '摸鱼', level3: '话痨', level4: '话满天', level5: '龙王', }; Waline.init({ el: '#waline', lang: 'en', reaction: true, serverURL: 'https://example.yunpengtai.top/', emoji: [ 'https://gcore.jsdelivr.net/gh/norevi/waline-blobcatemojis@1.0/blobs', 'https://unpkg.com/@waline/emojis@1.1.0/tw-emoji', 'https://unpkg.com/@waline/emojis@1.1.0/bmoji' ], reaction:[ 'https://gcore.jsdelivr.net/gh/norevi/waline-blobcatemojis@1.0/blobs/ablobcatheart.png', 'https://gcore.jsdelivr.net/gh/norevi/waline-blobcatemojis@1.0/blobs/ablobcatrainbow.png', 'https://gcore.jsdelivr.net/gh/norevi/waline-blobcatemojis@1.0/blobs/ablobcatattentionreverse.png', 'https://gcore.jsdelivr.net/gh/norevi/waline-blobcatemojis@1.0/blobs/ablobcatheartbroken.png', 'https://gcore.jsdelivr.net/gh/norevi/waline-blobcatemojis@1.0/blobs/blobcatopenmouth.png', 'https://gcore.jsdelivr.net/gh/norevi/waline-blobcatemojis@1.0/blobs/blobcatcoffee.png' ], dark: "__waline__css__", locale, }); let head = document.getElementsByTagName("head")[0]; let css = head.lastChild; let cssContent = css.textContent.replace("__waline__css__", ""); let cssContentPerferredDark = "@media (prefers-color-scheme: dark){html:not([data-mode])" + cssContent + "}"; let cssContentSelectedDark = "html[data-mode=dark]" + cssContent; css.textContent = cssContentPerferredDark; let style = document.createElement('style'); style.textContent = cssContentSelectedDark; head.appendChild(style); }); </script></div></div><footer class="row pl-3 pr-3"><div class="col-12 d-flex justify-content-between align-items-center text-muted pl-0 pr-0"><div class="footer-left"><p class="mb-0"> © 2023 <a href="https://twitter.com/TonySta14611077">Yunpeng Tai</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/1st-post/">1st post</a> <a class="post-tag" href="/tags/dataparallel/">DataParallel</a> <a class="post-tag" href="/tags/distributed-training/">Distributed Training</a> <a class="post-tag" href="/tags/experience/">Experience</a> <a class="post-tag" href="/tags/gradients/">Gradients</a> <a class="post-tag" href="/tags/hello-world/">hello world</a> <a class="post-tag" href="/tags/mathematics/">Mathematics</a> <a class="post-tag" href="/tags/multi-gpus/">Multi-gpus</a> <a class="post-tag" href="/tags/neural-networks/">Neural Networks</a> <a class="post-tag" href="/tags/optimization/">Optimization</a></div></div></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a><div id="notification" class="toast" role="alert" aria-live="assertive" aria-atomic="true" data-animation="true" data-autohide="false"><div class="toast-header"> <button type="button" class="ml-2 ml-auto close" data-dismiss="toast" aria-label="Close"> <span aria-hidden="true">&times;</span> </button></div><div class="toast-body text-center pt-0"><p class="pl-2 pr-2 mb-3">A new version of content is available.</p><button type="button" class="btn btn-primary" aria-label="Update"> Update </button></div></div><script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No results found.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script> <script src="https://cdn.jsdelivr.net/combine/npm/magnific-popup@1/dist/jquery.magnific-popup.min.js,npm/lozad/dist/lozad.min.js,npm/clipboard@2/dist/clipboard.min.js"></script> <script src="https://cdn.jsdelivr.net/combine/npm/dayjs@1/dayjs.min.js,npm/dayjs@1/locale/en.min.js,npm/dayjs@1/plugin/relativeTime.min.js,npm/dayjs@1/plugin/localizedFormat.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script> /* see: <https://docs.mathjax.org/en/latest/options/input/tex.html#tex-options> */ MathJax = { tex: { inlineMath: [ /* start/end delimiter pairs for in-line math */ ['$','$'], ['\\(','\\)'] ], displayMath: [ /* start/end delimiter pairs for display math */ ['$$', '$$'], ['\\[', '\\]'] ] } }; </script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"> </script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/js/bootstrap.bundle.min.js"></script> <script defer src="/app.js"></script> <script defer src="https://www.googletagmanager.com/gtag/js?id=G-Y3CX2RWEDY"></script> <script> document.addEventListener("DOMContentLoaded", function(event) { window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'G-Y3CX2RWEDY'); }); </script>
