<feed xmlns="http://www.w3.org/2005/Atom"> <id>https://www.yunpengtai.top/</id><title>Yunpeng Tai</title><subtitle>Stay Hungry. Stay Foolish.</subtitle> <updated>2023-02-04T14:23:43+08:00</updated> <author> <name>Yunpeng Tai</name> <uri>https://www.yunpengtai.top/</uri> </author><link rel="self" type="application/atom+xml" href="https://www.yunpengtai.top/feed.xml"/><link rel="alternate" type="text/html" hreflang="en" href="https://www.yunpengtai.top/"/> <generator uri="https://jekyllrb.com/" version="4.2.2">Jekyll</generator> <rights> © 2023 Yunpeng Tai </rights> <icon>/assets/img/favicons/favicon.ico</icon> <logo>/assets/img/favicons/favicon-96x96.png</logo> <entry><title>Diving in distributed training in PyTorch</title><link href="https://www.yunpengtai.top/posts/Diving-in-distributed-training/" rel="alternate" type="text/html" title="Diving in distributed training in PyTorch" /><published>2022-11-20T21:37:00+08:00</published> <updated>2022-11-22T09:23:15+08:00</updated> <id>https://www.yunpengtai.top/posts/Diving-in-distributed-training/</id> <content src="https://www.yunpengtai.top/posts/Diving-in-distributed-training/" /> <author> <name>Yunpeng Tai</name> </author> <category term="Training" /> <category term="PyTorch" /> <summary> 鉴于网上此类教程有不少模糊不清，对原理不得其法，代码也难跑通，故而花了几天细究了一下相关原理和实现，欢迎批评指正！ 关于此部分的代码，可以去这里查看 在开始前，我需要特别致谢一下一位挚友，他送了我双显卡的机器来赞助我做个人研究，否则多卡的相关实验就得付费在云平台上跑了，感谢好朋友一路以来的支持，这份恩情值得一辈子铭记！这篇文章作为礼物赠与挚友。 Why Parallel 我们在两种情况下进行并行化训练1： 模型一张卡放不下：我们需要将模型不同的结构放置到不同的GPU上运行，这种情况叫ModelParallel(MP) 一张卡的batch size(bs)过小：有些时候数据的最大长度调的比较高（e.g., 512），可用的bs就很小，较小的bs会导致收敛不稳定，因而将数据分发到多个GPU上进行并行训练，这种情况叫DataParallel(DP)。当然，DP肯... </summary> </entry> <entry><title>Going Deeper into Back-propagation</title><link href="https://www.yunpengtai.top/posts/Back-propagation/" rel="alternate" type="text/html" title="Going Deeper into Back-propagation" /><published>2022-09-07T11:10:00+08:00</published> <updated>2023-02-04T14:22:51+08:00</updated> <id>https://www.yunpengtai.top/posts/Back-propagation/</id> <content src="https://www.yunpengtai.top/posts/Back-propagation/" /> <author> <name>Yunpeng Tai</name> </author> <category term="Training" /> <category term="Deep Learning" /> <summary> 1. Gradient descent optimization Gradient-based methods make use of the gradient information to adjust the parameters. Among them, gradient descent can be the simplest. Gradient descent makes the parameters to walk a small step in the direction of the negative gradient. \[\boldsymbol{w}^{\tau + 1} = \boldsymbol{w}^{\tau} - \eta \nabla_{\boldsymbol{w}^{\tau}} E \tag{1.1}\] where $\eta, \tau, ... </summary> </entry> <entry><title>Tips for Training Neural Networks</title><link href="https://www.yunpengtai.top/posts/Tips-for-Training-Neural-Networks/" rel="alternate" type="text/html" title="Tips for Training Neural Networks" /><published>2022-07-30T19:43:00+08:00</published> <updated>2022-08-01T14:28:05+08:00</updated> <id>https://www.yunpengtai.top/posts/Tips-for-Training-Neural-Networks/</id> <content src="https://www.yunpengtai.top/posts/Tips-for-Training-Neural-Networks/" /> <author> <name>Yunpeng Tai</name> </author> <category term="Training" /> <category term="Deep Learning" /> <summary> Recently, I have read a blog about training neural networks (simplified as NN in the rest part of this post) and it is really amazing. I am going to add my own experience in this post along with summarizing that blog’s interesting part. Nowadays, it seems like that training NN is extremely easy for there are plenty of free frameworks which are simple to use (e.g. PyTorch, Numpy, Tensorflow). W... </summary> </entry> <entry><title>Quotes of Mathematicians</title><link href="https://www.yunpengtai.top/posts/Quotes-of-Mathematicians/" rel="alternate" type="text/html" title="Quotes of Mathematicians" /><published>2022-07-23T09:56:00+08:00</published> <updated>2022-07-23T17:17:17+08:00</updated> <id>https://www.yunpengtai.top/posts/Quotes-of-Mathematicians/</id> <content src="https://www.yunpengtai.top/posts/Quotes-of-Mathematicians/" /> <author> <name>Yunpeng Tai</name> </author> <category term="Quotes" /> <summary> Life is complex, and it has both real and imaginary parts. — Someone Basically, I’m not interested in doing research and I never have been… I’m interested in understanding, which is quite a different thing. And often to understand something you have to work it out yourself because no one else has done it. — David Blackwell To not know maths is a severe limitation to understanding the wor... </summary> </entry> <entry><title>Retrieval-Enhanced Transformer</title><link href="https://www.yunpengtai.top/posts/Retieval-Enhanced-transformer/" rel="alternate" type="text/html" title="Retrieval-Enhanced Transformer" /><published>2022-06-19T21:06:00+08:00</published> <updated>2022-07-22T14:22:13+08:00</updated> <id>https://www.yunpengtai.top/posts/Retieval-Enhanced-transformer/</id> <content src="https://www.yunpengtai.top/posts/Retieval-Enhanced-transformer/" /> <author> <name>Yunpeng Tai</name> </author> <category term="PLMs" /> <summary> Problems To Solve To Scale Down the model size while maintaining the performances. To incorporate External Memory Retrieval in the Large Language Model Modeling. How? Data Construction Training &amp;amp; Evaluation set: $\text{MassiveText}$ for both training &amp;amp; retrieval data (contains 5 trillion tokens) SentencePiece with a vocabulary of $128K$ tokens Durin... </summary> </entry> </feed>
